{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch notebook: The nitty-gritty\n",
    "\n",
    "I hope to one day make this into a lovely notebook that gets into the nitty-gritty of Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Layers**\n",
    "\n",
    "* `nn.Linear(input_features, output_features)` performs the transformation:\n",
    "    - $Y = X W^{T} + b $, where:\n",
    "    - $Y$ is (batch_size, output_features), here output_features > 1, if we are trying to predict >1 outcome.\n",
    "    - $X$ is (batch_size, input features)\n",
    "    - $W^{T}$ is (input_features, output_features)\n",
    "* `nn.Linear(...)` returns $Y$.\n",
    "* When you return the weight matrix of `nn.Linear(...)` it's shape is (output_features, input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m in_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      3\u001b[0m out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m----> 5\u001b[0m linear \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mLinear(in_features, out_features)\n\u001b[1;32m      7\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(batch_size, in_features)\n\u001b[1;32m      9\u001b[0m Y \u001b[38;5;241m=\u001b[39m linear(X)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "in_features = 5\n",
    "out_features = 3\n",
    "\n",
    "linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "X = torch.rand(batch_size, in_features)\n",
    "\n",
    "Y = linear(X)\n",
    "\n",
    "print(\"X...\")\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Y...\")\n",
    "print(Y)\n",
    "print(Y.shape)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Linear Weight...\")\n",
    "print(linear.weight)\n",
    "print(linear.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN layer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([5, 1, 3])\n",
      "Output shape: torch.Size([5, 1, 3])\n",
      "Hidden shape: torch.Size([1, 1, 3])\n",
      "hh outputs the hidden shape ONLY for the final time step\n",
      "tensor([[[-0.1781,  0.9003,  0.6967]]], grad_fn=<StackBackward0>)\n",
      "\n",
      "hh outputs the hidden shape for all time steps\n",
      "tensor([[[ 0.4643,  0.8609,  0.6027]],\n",
      "\n",
      "        [[-0.0283,  0.9431,  0.6206]],\n",
      "\n",
      "        [[-0.1864,  0.9239,  0.6808]],\n",
      "\n",
      "        [[-0.0794,  0.9508,  0.7809]],\n",
      "\n",
      "        [[-0.1781,  0.9003,  0.6967]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_size = 3\n",
    "seq_length = 5\n",
    "batch_size = 1\n",
    "\n",
    "rnn = nn.RNN(input_size=input_size, hidden_size=3)\n",
    "\n",
    "X = torch.rand(seq_length, batch_size, input_size)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "\n",
    "out, hh = rnn(X)\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Hidden shape: {hh.shape}\")\n",
    "print(\"hh outputs the hidden shape ONLY for the final time step\")\n",
    "print(hh)\n",
    "print(\"\")\n",
    "print(\"hh outputs the hidden shape for all time steps\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `loss_cat = nn.CrossEntropyLoss(y_hat,y)` is cross entropy loss. Inputs should be the length of the number of categories. y_hat should be logits (output from linear layer), y should be ground truth labels\n",
    "* $y_{pred}$ should be (batch_size, num_features, num_classes)\n",
    "* $y_{true}$ should (batch_size, num features), with the correct label in the num_features column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5966,  0.2707,  0.1667,  1.3254,  0.0822],\n",
       "        [-0.4303, -0.3144, -0.1222,  0.1305, -0.5695],\n",
       "        [ 0.6989, -0.0115, -1.4339,  1.8006, -0.5688]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1327, 0.0646, 0.4527, 0.2125, 0.1376],\n",
       "        [0.0821, 0.2198, 0.3922, 0.1196, 0.1864],\n",
       "        [0.2808, 0.3997, 0.1361, 0.0798, 0.1036]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few extra notes\n",
    "\n",
    "* Be careful when overwriting tensor like `tensor_a[:] = tensor_b[:]`. This can mess up the computation graph and will not allow you to gradient descend "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLEconPS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
