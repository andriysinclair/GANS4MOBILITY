{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import logging\n",
    "from Modules.Loader_wrangler import *\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure basic logging\n",
    "logging.basicConfig(level=logging.INFO, force=True, format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = loader(output_file_name=\"merged_df2017.pkl\", chunksize=100000, sample_size=100000, survey_year=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = pd.read_pickle(\"/home/trapfishscott/Cambridge24.25/D200_ML_econ/ProblemSets/Project/data/merged_df2017.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining only relevant variables and making into a time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation pipeline\n",
    "\n",
    "1. One-hot encode categorical features + any small cleaning steps\n",
    "2. Add days of the week with no car travel\n",
    "3. Make data frame into wide format\n",
    "4. Convert to tensor\n",
    "\n",
    "* Includes JourSeq gaps if trips were made by non-car inbetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### small cleaning steps and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporal_vars = [\"TWSMonth\", \"TravelYear\", \"TravelWeekDay_B01ID\"]\n",
    "#individual_vars =[\"PSUGOR_B02ID\", \"IndIncome2002_B02ID\", \"HHoldNumChildren\", \"DVLALengthBand_B01ID\"]\n",
    "\n",
    "numerical_outcome_vars = [\"TripStart\", \"TripEnd\", \"TripDisExSW\"]\n",
    "categorical_outcome_vars = [\"TripPurpose_B01ID\"]\n",
    "\n",
    "\n",
    "extra_vars = [\"IndividualID_x\", \"JourSeq\"]\n",
    "\n",
    "\n",
    "features_one_hot = [\"PSUGOR_B02ID\"]\n",
    "features_numerical = [\"TravelYear\", \"HHoldNumChildren\", \"IndIncome2002_B02ID\", \"DVLALengthBand_B01ID\"]\n",
    "features_cyclical = [\"TWSMonth\", \"TravelWeekDay_B01ID\"]\n",
    "\n",
    "features = features_one_hot + features_numerical + features_cyclical\n",
    "outcomes = numerical_outcome_vars + categorical_outcome_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = play[extra_vars +  features + outcomes]\n",
    "ts_df = ts_df.sort_values([\"IndividualID_x\", \"TravelWeekDay_B01ID\", \"JourSeq\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical encoder\n",
    "\n",
    "def apply_cyclical_encoding(column, type_, max_val):\n",
    "\n",
    "    if type_ == \"cos\":\n",
    "        return np.cos(2 * np.pi * column/ max_val)\n",
    "    else:\n",
    "        return np.sin(2 * np.pi * column/ max_val)\n",
    "\n",
    "\n",
    "def custom_numerical_scaler(x, x_min, x_max, inverse=False):\n",
    "    if not inverse:\n",
    "        x_scaled = (x-x_min)/(x_max - x_min)\n",
    "        return x_scaled\n",
    "    else:\n",
    "        x_unscaled = x*(x_max - x_min) + x_min\n",
    "        return x_unscaled\n",
    "\n",
    "\n",
    "standard_mms = MinMaxScaler()\n",
    "\n",
    "def log_transformer(x, inverse=False):\n",
    "    if not inverse:\n",
    "        return np.log1p(x)\n",
    "    else:\n",
    "        return np.expm1(x)\n",
    "    \n",
    "# Apply one-hot to categorical\n",
    "ohe = OneHotEncoder(sparse_output=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nohe_df = pd.DataFrame(ohe_array, columns=ohe.get_feature_names_out(features_one_hot))\\n\\n# Reset index to avoid misalignment\\nts_df.reset_index(drop=True, inplace=True)\\nohe_df.reset_index(drop=True, inplace=True)\\n\\ndf = pd.concat([ts_df, ohe_df], axis=1)'\\n\""
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply cyclical encoding to cyclical column\n",
    "\n",
    "ts_df.loc[:, \"TravelWeekDay_B01ID\"] = ts_df.loc[:, \"TravelWeekDay_B01ID\"].astype(int)\n",
    "\n",
    "standard_mms.fit_transform(ts_df[features_numerical])\n",
    "\n",
    "# Careful not to run twice\n",
    "\n",
    "for col in features_one_hot:\n",
    "    ts_df[col] = ts_df[col].astype(int)\n",
    "\n",
    "ohe.fit_transform(ts_df[features_one_hot])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing travel days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_travel_week_for_i(i_df, i_id, full_week_encoding, features=features, outcomes=outcomes):\n",
    "        \n",
    "    break_flag = False\n",
    "\n",
    "    # Travel days with travel \n",
    "    included_travel_day = i_df[\"TravelWeekDay_B01ID\"].to_list()\n",
    "\n",
    "    # Travel days with no travel\n",
    "    travel_day_no_drive = list(set(full_week_encoding) - set(included_travel_day))\n",
    "\n",
    "    # These values will repeat for empty-travel travel days\n",
    "    imputed_travel_df = pd.DataFrame({\n",
    "        \"TravelWeekDay_B01ID\": travel_day_no_drive,\n",
    "        \"IndividualID_x\": [i_id]*len(travel_day_no_drive),\n",
    "        \"JourSeq\": [1]*len(travel_day_no_drive)\n",
    "    })\n",
    "\n",
    "\n",
    "    \n",
    "    # Looping through all the columns in the original df\n",
    "    for col in i_df.columns:\n",
    "\n",
    "        # For days with no travel all outcomes vars will take 0\n",
    "        if col in outcomes:\n",
    "            imputed_travel_df[col] = [0]*len(travel_day_no_drive)\n",
    "\n",
    "    \n",
    "        else:\n",
    "        \n",
    "            if col not in extra_vars + [\"TravelWeekDay_B01ID\"]:\n",
    "                if len(i_df[col].unique()) != 1:\n",
    "                    print(f\"{col} is erroneous for {i_id}\")\n",
    "                    print(f\"Unique vals: {i_df[col].unique()}\")\n",
    "                    break_flag = True\n",
    "                    break\n",
    "                else:\n",
    "                    imputed_travel_df[col] = i_df[col].unique()[0]\n",
    "\n",
    "    if break_flag:\n",
    "        print(\"Continuing to next individual\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    #display(imputed_travel_df)\n",
    "    #display(i_df)\n",
    "\n",
    "    # Merging on IndividualID_x and TravelWeekDay_B01ID\n",
    "    #full_df = i_df.merge(imputed_travel_df, on=[\"IndividualID_x\", \"TravelWeekDay_B01ID\"], how=\"left\")\n",
    "\n",
    "    # Concatenating df to include empty travel days\n",
    "    full_df = pd.concat([i_df, imputed_travel_df])\n",
    "    #display(full_df)\n",
    "\n",
    "    full_df = full_df.sort_values([\"TravelYear\", \"TWSMonth\", \"TravelWeekDay_B01ID\", \"JourSeq\", \"TripStart\", \"TripEnd\"])\n",
    "\n",
    "    full_df.loc[:, \"TWSMonth_cos\"] = apply_cyclical_encoding(column=full_df[\"TWSMonth\"], type_=\"cos\", max_val=12)\n",
    "    full_df.loc[:, \"TWSMonth_sin\"] = apply_cyclical_encoding(column=full_df[\"TWSMonth\"], type_=\"sin\", max_val=12)\n",
    "\n",
    "    full_df.loc[:, \"TravelWeekDay_B01ID_cos\"] = apply_cyclical_encoding(column=full_df[\"TravelWeekDay_B01ID\"], type_=\"cos\", max_val=7)\n",
    "    full_df.loc[:, \"TravelWeekDay_B01ID_sin\"] = apply_cyclical_encoding(column=full_df[\"TravelWeekDay_B01ID\"], type_=\"sin\", max_val=7)\n",
    "\n",
    "    if full_df[\"TripStart\"].max() > 1.5:\n",
    "        full_df.loc[:,\"TripStart\"] = full_df[\"TripStart\"].apply(lambda x: custom_numerical_scaler(x, x_max=60*24, x_min=0))\n",
    "\n",
    "    if full_df[\"TripEnd\"].max() > 1.5:\n",
    "        full_df.loc[:,\"TripEnd\"] = full_df[\"TripEnd\"].apply(lambda x: custom_numerical_scaler(x, x_max=60*24, x_min=0))\n",
    "\n",
    "    full_df.loc[:,\"TripDisExSW\"] = full_df.loc[:,\"TripDisExSW\"].apply(lambda x: log_transformer(x))\n",
    "\n",
    "    full_df.loc[:,features_numerical] = standard_mms.transform(full_df[features_numerical])\n",
    "\n",
    "    ohe_array = ohe.transform(full_df[features_one_hot])\n",
    "    ohe_df = pd.DataFrame(ohe_array, columns=ohe.get_feature_names_out(features_one_hot))\n",
    "\n",
    "    # Reset index to avoid misalignment\n",
    "    full_df.reset_index(drop=True, inplace=True)\n",
    "    ohe_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    full_df = pd.concat([full_df, ohe_df], axis=1)\n",
    "\n",
    "    #display(full_df)\n",
    "\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming to wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_wide_for_i(i_df, max_journey_seq, seq_length = 7, outcomes=outcomes, features=features, extra_vars=extra_vars):\n",
    "    df = i_df.copy()\n",
    "\n",
    "    expected_all = [f\"{col}_{i}\" for col in outcomes for i in range(1, max_journey_seq+1)]\n",
    "    expected_categorical = [f\"{col}_{i}\" for col in categorical_outcome_vars for i in range(1, max_journey_seq+1)]\n",
    "\n",
    "    df = df[df[\"JourSeq\"]<=max_journey_seq]\n",
    "\n",
    "    #\n",
    "\n",
    "    df_wide = df.pivot(index=\"TravelWeekDay_B01ID\",\n",
    "                  columns = \"JourSeq\",\n",
    "                  values = outcomes)\n",
    "    \n",
    "    df_wide.columns = [f\"{col[0]}_{int(col[1])}\" for col in df_wide.columns]\n",
    "\n",
    "    for col in expected_all:\n",
    "        if col not in df_wide.columns:\n",
    "            df_wide[col] = 0\n",
    "    \n",
    "    # Ensure column order is consistent\n",
    "    df_wide = df_wide[expected_all]\n",
    "    \n",
    "    df_wide = df_wide.fillna(0)\n",
    "\n",
    "    df_wide.reset_index(inplace=True)\n",
    "\n",
    "    # Dropping outcome columns\n",
    "    df.drop(columns=outcomes + extra_vars, axis=1, inplace = True)\n",
    "    df.drop_duplicates(subset=[\"TravelWeekDay_B01ID\"], inplace=True)\n",
    "\n",
    "    df_wide = df_wide.merge(df, on=\"TravelWeekDay_B01ID\", how=\"left\")\n",
    "\n",
    "    top_row = df_wide.head(1).copy()\n",
    "\n",
    "    for col in expected_all:\n",
    "        top_row[col] = 0\n",
    "        top_row[\"TravelWeekDay_B01ID\"] = 0\n",
    "\n",
    "    repeated_rows = pd.concat([top_row] * seq_length, ignore_index=True)\n",
    "\n",
    "    df_wide = pd.concat([repeated_rows, df_wide], ignore_index=True)\n",
    "\n",
    "    df_wide.drop(columns=features_one_hot + features_cyclical, inplace=True, axis=1)\n",
    "\n",
    "    #df_wide.drop(columns=features_cyclical + features_one_hot, axis=1, inplace=True)\n",
    "\n",
    "    targets_only = df_wide.drop(columns=features + extra_vars, axis=1, errors=\"ignore\")\n",
    "\n",
    "    targets_only = targets_only.iloc[seq_length:,:]\n",
    "\n",
    "    targets_cont = targets_only[expected_all]\n",
    "    targets_cont = targets_cont.copy()\n",
    "    targets_cont.drop(columns=expected_categorical, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    targets_cat = targets_only[expected_categorical]\n",
    "\n",
    "    return df_wide, targets_cont, targets_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting altogether for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_LSTM(long_df, impute_missing_travel_weeks=True, transform_to_wide=False, transform_to_tensor=False, debug=False):\n",
    "\n",
    "    df = long_df.copy()\n",
    "           \n",
    "\n",
    "    #df = df[~df[\"DVLALengthBand_B01ID\"].isin([-8, -10])]\n",
    "\n",
    "    # All unique individual id's to loop over\n",
    "    individual_ids = df[\"IndividualID_x\"].unique()\n",
    "\n",
    "    # Apply numerical encoding to numerical column\n",
    "    #\n",
    "\n",
    "    df_chunks = []\n",
    "\n",
    "    full_week_encoding = list(range(1,8))\n",
    "\n",
    "    if debug:\n",
    "        random_index = random.randint(0, len(individual_ids))\n",
    "\n",
    "        debug_df = df[df[\"IndividualID_x\"] == individual_ids[random_index]]\n",
    "\n",
    "        #display(debug_df)\n",
    "\n",
    "        debug_df = impute_missing_travel_week_for_i(debug_df, i_id=individual_ids[random_index], full_week_encoding=full_week_encoding)\n",
    "\n",
    "        #display(debug_df)\n",
    "\n",
    "        debug_df, debug_targets_cont, debug_targets_cat = transform_to_wide_for_i(debug_df, max_journey_seq=10)\n",
    "\n",
    "        for i, col in enumerate(debug_df.columns):\n",
    "            print(f\"{i}: {col}\")\n",
    "        print(\"\")\n",
    "        for i, col in enumerate(debug_targets_cont.columns):\n",
    "            print(f\"{i}: {col}\")\n",
    "        print(\"\")\n",
    "        for i, col in enumerate(debug_targets_cat.columns):\n",
    "            print(f\"{i}: {col}\")\n",
    "\n",
    "        #print(debug_df.iloc[:,[0,1,2,3,20,31]].to_latex())\n",
    "\n",
    "        #display(debug_df.iloc[0:7,[0,1,2,46]])\n",
    "\n",
    "        #display(debug_df)\n",
    "\n",
    "        #display(debug_targets_cont)\n",
    "\n",
    "        #display(debug_targets_cat)\n",
    "\n",
    "        return\n",
    "    \n",
    "    if transform_to_tensor:\n",
    "        individual_tensors = []\n",
    "        target_cont_tensors = []\n",
    "        target_cat_tensors = []\n",
    "    \n",
    "    if impute_missing_travel_weeks:\n",
    "\n",
    "        for i, individual_id in enumerate(individual_ids[:]):\n",
    "\n",
    "            i_df = df[df[\"IndividualID_x\"] == individual_id]\n",
    "\n",
    "            full_df = impute_missing_travel_week_for_i(i_df, i_id=individual_id, full_week_encoding=full_week_encoding)\n",
    "\n",
    "            #display(full_df)\n",
    "\n",
    "            if full_df is not None:\n",
    "                if not transform_to_wide:\n",
    "                    df_chunks.append(full_df)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    full_df, targets_cont, targets_cat = transform_to_wide_for_i(full_df, max_journey_seq=10)\n",
    "                    \n",
    "                    if transform_to_tensor:\n",
    "\n",
    "                    \n",
    "                        full_arr = full_df.to_numpy()\n",
    "                        full_arr = np.expand_dims(full_arr, axis=1)\n",
    "\n",
    "                        targets_cont_arr = targets_cont.to_numpy()\n",
    "                        targets_cat_arr = targets_cat.to_numpy()\n",
    "\n",
    "                        full_i_tensor = torch.tensor(full_arr)\n",
    "                        target_cont_i_tensor = torch.tensor(targets_cont_arr)\n",
    "                        target_cat_i_tensor = torch.tensor(targets_cat_arr)\n",
    "\n",
    "                        individual_tensors.append(full_i_tensor)\n",
    "                        target_cont_tensors.append(target_cont_i_tensor)\n",
    "                        target_cat_tensors.append(target_cat_i_tensor)\n",
    "\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        #display(full_df)\n",
    "                        print(\"\")\n",
    "                        #display(targets)\n",
    "                        df_chunks.append(full_df)\n",
    "\n",
    "            sys.stdout.write(f\"\\rIndividual {i+1} out of {len(individual_ids)} Complete!    \")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        if transform_to_tensor:\n",
    "            individual_tensors = torch.stack(individual_tensors, dim=0)\n",
    "            target_cont_tensors = torch.stack(target_cont_tensors, dim=0)\n",
    "            target_cat_tensors = torch.stack(target_cat_tensors, dim=0)\n",
    "            return individual_tensors, target_cont_tensors, target_cat_tensors\n",
    "        \n",
    "        else:\n",
    "\n",
    "            df_to_return = pd.concat(df_chunks)\n",
    "\n",
    "            return df_to_return\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: TripStart_1\n",
      "1: TripStart_2\n",
      "2: TripStart_3\n",
      "3: TripStart_4\n",
      "4: TripStart_5\n",
      "5: TripStart_6\n",
      "6: TripStart_7\n",
      "7: TripStart_8\n",
      "8: TripStart_9\n",
      "9: TripStart_10\n",
      "10: TripEnd_1\n",
      "11: TripEnd_2\n",
      "12: TripEnd_3\n",
      "13: TripEnd_4\n",
      "14: TripEnd_5\n",
      "15: TripEnd_6\n",
      "16: TripEnd_7\n",
      "17: TripEnd_8\n",
      "18: TripEnd_9\n",
      "19: TripEnd_10\n",
      "20: TripDisExSW_1\n",
      "21: TripDisExSW_2\n",
      "22: TripDisExSW_3\n",
      "23: TripDisExSW_4\n",
      "24: TripDisExSW_5\n",
      "25: TripDisExSW_6\n",
      "26: TripDisExSW_7\n",
      "27: TripDisExSW_8\n",
      "28: TripDisExSW_9\n",
      "29: TripDisExSW_10\n",
      "30: TripPurpose_B01ID_1\n",
      "31: TripPurpose_B01ID_2\n",
      "32: TripPurpose_B01ID_3\n",
      "33: TripPurpose_B01ID_4\n",
      "34: TripPurpose_B01ID_5\n",
      "35: TripPurpose_B01ID_6\n",
      "36: TripPurpose_B01ID_7\n",
      "37: TripPurpose_B01ID_8\n",
      "38: TripPurpose_B01ID_9\n",
      "39: TripPurpose_B01ID_10\n",
      "40: TravelYear\n",
      "41: HHoldNumChildren\n",
      "42: IndIncome2002_B02ID\n",
      "43: DVLALengthBand_B01ID\n",
      "44: TWSMonth_cos\n",
      "45: TWSMonth_sin\n",
      "46: TravelWeekDay_B01ID_cos\n",
      "47: TravelWeekDay_B01ID_sin\n",
      "48: PSUGOR_B02ID_1\n",
      "49: PSUGOR_B02ID_2\n",
      "50: PSUGOR_B02ID_3\n",
      "51: PSUGOR_B02ID_4\n",
      "52: PSUGOR_B02ID_5\n",
      "53: PSUGOR_B02ID_6\n",
      "54: PSUGOR_B02ID_7\n",
      "55: PSUGOR_B02ID_8\n",
      "56: PSUGOR_B02ID_9\n",
      "\n",
      "0: TripStart_1\n",
      "1: TripStart_2\n",
      "2: TripStart_3\n",
      "3: TripStart_4\n",
      "4: TripStart_5\n",
      "5: TripStart_6\n",
      "6: TripStart_7\n",
      "7: TripStart_8\n",
      "8: TripStart_9\n",
      "9: TripStart_10\n",
      "10: TripEnd_1\n",
      "11: TripEnd_2\n",
      "12: TripEnd_3\n",
      "13: TripEnd_4\n",
      "14: TripEnd_5\n",
      "15: TripEnd_6\n",
      "16: TripEnd_7\n",
      "17: TripEnd_8\n",
      "18: TripEnd_9\n",
      "19: TripEnd_10\n",
      "20: TripDisExSW_1\n",
      "21: TripDisExSW_2\n",
      "22: TripDisExSW_3\n",
      "23: TripDisExSW_4\n",
      "24: TripDisExSW_5\n",
      "25: TripDisExSW_6\n",
      "26: TripDisExSW_7\n",
      "27: TripDisExSW_8\n",
      "28: TripDisExSW_9\n",
      "29: TripDisExSW_10\n",
      "\n",
      "0: TripPurpose_B01ID_1\n",
      "1: TripPurpose_B01ID_2\n",
      "2: TripPurpose_B01ID_3\n",
      "3: TripPurpose_B01ID_4\n",
      "4: TripPurpose_B01ID_5\n",
      "5: TripPurpose_B01ID_6\n",
      "6: TripPurpose_B01ID_7\n",
      "7: TripPurpose_B01ID_8\n",
      "8: TripPurpose_B01ID_9\n",
      "9: TripPurpose_B01ID_10\n"
     ]
    }
   ],
   "source": [
    "df = prepare_data_for_LSTM(long_df=ts_df, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 5949 out of 6838 Complete!    TravelYear is erroneous for 2017014397.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 5950 out of 6838 Complete!    TravelYear is erroneous for 2017014398.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 5998 out of 6838 Complete!    TravelYear is erroneous for 2017014552.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6057 out of 6838 Complete!    TravelYear is erroneous for 2017014714.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6058 out of 6838 Complete!    TravelYear is erroneous for 2017014715.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6086 out of 6838 Complete!    TravelYear is erroneous for 2017014773.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6164 out of 6838 Complete!    TravelYear is erroneous for 2017014964.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6165 out of 6838 Complete!    TravelYear is erroneous for 2017014965.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6201 out of 6838 Complete!    TravelYear is erroneous for 2017015043.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6202 out of 6838 Complete!    TravelYear is erroneous for 2017015044.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6203 out of 6838 Complete!    TravelYear is erroneous for 2017015045.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6204 out of 6838 Complete!    TravelYear is erroneous for 2017015046.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6227 out of 6838 Complete!    TravelYear is erroneous for 2017015097.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6242 out of 6838 Complete!    TravelYear is erroneous for 2017015137.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6248 out of 6838 Complete!    TravelYear is erroneous for 2017015147.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6259 out of 6838 Complete!    TravelYear is erroneous for 2017015165.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6273 out of 6838 Complete!    TravelYear is erroneous for 2017015191.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6274 out of 6838 Complete!    TravelYear is erroneous for 2017015192.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6275 out of 6838 Complete!    TravelYear is erroneous for 2017015193.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6277 out of 6838 Complete!    TravelYear is erroneous for 2017015196.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6298 out of 6838 Complete!    TravelYear is erroneous for 2017015249.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6304 out of 6838 Complete!    TravelYear is erroneous for 2017015258.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6328 out of 6838 Complete!    TravelYear is erroneous for 2017015305.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6347 out of 6838 Complete!    TravelYear is erroneous for 2017015342.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6353 out of 6838 Complete!    TravelYear is erroneous for 2017015362.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6365 out of 6838 Complete!    TravelYear is erroneous for 2017015387.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6366 out of 6838 Complete!    TravelYear is erroneous for 2017015388.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6368 out of 6838 Complete!    TravelYear is erroneous for 2017015390.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6372 out of 6838 Complete!    TravelYear is erroneous for 2017015397.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6373 out of 6838 Complete!    TravelYear is erroneous for 2017015398.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6410 out of 6838 Complete!    TravelYear is erroneous for 2017015477.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6411 out of 6838 Complete!    TravelYear is erroneous for 2017015479.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6418 out of 6838 Complete!    TravelYear is erroneous for 2017015496.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6440 out of 6838 Complete!    TravelYear is erroneous for 2017015562.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6443 out of 6838 Complete!    TravelYear is erroneous for 2017015569.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6449 out of 6838 Complete!    TravelYear is erroneous for 2017015579.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6450 out of 6838 Complete!    TravelYear is erroneous for 2017015581.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6451 out of 6838 Complete!    TravelYear is erroneous for 2017015582.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6454 out of 6838 Complete!    TravelYear is erroneous for 2017015588.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6456 out of 6838 Complete!    TravelYear is erroneous for 2017015590.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6465 out of 6838 Complete!    TravelYear is erroneous for 2017015602.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6471 out of 6838 Complete!    TravelYear is erroneous for 2017015624.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6472 out of 6838 Complete!    TravelYear is erroneous for 2017015625.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6479 out of 6838 Complete!    TravelYear is erroneous for 2017015642.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6481 out of 6838 Complete!    TravelYear is erroneous for 2017015644.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6485 out of 6838 Complete!    TravelYear is erroneous for 2017015659.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6486 out of 6838 Complete!    TravelYear is erroneous for 2017015667.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6487 out of 6838 Complete!    TravelYear is erroneous for 2017015668.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6488 out of 6838 Complete!    TravelYear is erroneous for 2017015671.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6495 out of 6838 Complete!    TravelYear is erroneous for 2017015701.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6496 out of 6838 Complete!    TravelYear is erroneous for 2017015704.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6500 out of 6838 Complete!    TravelYear is erroneous for 2017015726.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6514 out of 6838 Complete!    TravelYear is erroneous for 2017015756.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6519 out of 6838 Complete!    TravelYear is erroneous for 2017015765.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6521 out of 6838 Complete!    TravelYear is erroneous for 2017015770.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6544 out of 6838 Complete!    TravelYear is erroneous for 2017015800.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6549 out of 6838 Complete!    TravelYear is erroneous for 2017015811.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6564 out of 6838 Complete!    TravelYear is erroneous for 2017015860.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6565 out of 6838 Complete!    TravelYear is erroneous for 2017015862.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6568 out of 6838 Complete!    TravelYear is erroneous for 2017015867.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6569 out of 6838 Complete!    TravelYear is erroneous for 2017015868.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6573 out of 6838 Complete!    TravelYear is erroneous for 2017015915.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6594 out of 6838 Complete!    TravelYear is erroneous for 2017015980.0\n",
      "Unique vals: [1. 0.]\n",
      "Continuing to next individual\n",
      "Individual 6838 out of 6838 Complete!    "
     ]
    }
   ],
   "source": [
    "X, y_cont, y_cat = prepare_data_for_LSTM(long_df=ts_df, transform_to_wide=True, transform_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tensors\n",
    "with open(\"/home/trapfishscott/Cambridge24.25/D200_ML_econ/ProblemSets/Project/tensors/tensors.pkl\", \"wb\") as f:\n",
    "    pickle.dump((X, y_cont, y_cat), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tensors\n",
    "with open(\"/home/trapfishscott/Cambridge24.25/D200_ML_econ/ProblemSets/Project/tensors/tensors.pkl\", \"rb\") as f:\n",
    "    (X, y_cont, y_cat) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([6775, 14, 1, 57])\n",
      "Cont Output shape: torch.Size([6775, 7, 30])\n",
      "Cat Output shape: torch.Size([6775, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "X = X.to(torch.float32)\n",
    "y_cont = y_cont.to(torch.float32)\n",
    "y_cat = y_cat.to(torch.long)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Cont Output shape: {y_cont.shape}\")\n",
    "print(f\"Cat Output shape: {y_cat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "# Defining parameters\n",
    "INPUT_SIZE = X.shape[3]\n",
    "HIDDEN_SIZE = 3\n",
    "NUM_LAYERS = 1\n",
    "OUTPUT_SIZE_CONT = y_cont.shape[2]\n",
    "OUTPUT_SIZE_CAT = y_cat.shape[2]\n",
    "\n",
    "NUM_CLASSES = int(ts_df[\"TripPurpose_B01ID\"].max())+1\n",
    "print(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MaxValue that trip start/ end can take\n",
    "\n",
    "trip_time_max_val = custom_numerical_scaler(60*24, 0, 60*24)\n",
    "\n",
    "trip_time_max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define RNN layer\n",
    "\n",
    "        self.rnn = nn.RNN(INPUT_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "        # Output layers\n",
    "\n",
    "        self.output_cont = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE_CONT)\n",
    "        self.output_cat = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE_CAT)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        out, hh = self.rnn(X)\n",
    "\n",
    "        y_cont_hat_vector = self.output_cont(hh)\n",
    "\n",
    "        y_cat_hat= self.output_cat(hh)\n",
    "\n",
    "        y_cat_hat = y_cat_hat.permute(0,2,1)\n",
    "        y_cat_hat = torch.cat([y_cat_hat]*NUM_CLASSES, dim=2)\n",
    "        y_cat_hat = y_cat_hat.reshape(OUTPUT_SIZE_CAT, NUM_CLASSES)\n",
    "\n",
    "        # stacking downward NUM_CLASSES times\n",
    "        #y_cat_hat = y_cat_hat.repeat()\n",
    "        #print(y_cat_hat)\n",
    "\n",
    "        y_cont_hat = y_cont_hat_vector[0,0,:]\n",
    "\n",
    "        y_cont_hat = y_cont_hat.to(torch.float32)\n",
    "        y_purpouse_pred = y_cat_hat.to(torch.float32)\n",
    "\n",
    "        # appplying relu so that continous values are non-negative and maxed at 1\n",
    "        y_tripstart_pred = torch.clamp(y_cont_hat[:10], min=0, max=1)\n",
    "        y_tripend_pred = torch.clamp(y_cont_hat[10:20], min=0)\n",
    "        y_distance_pred = y_cont_hat[20:]\n",
    "\n",
    "        # Applying a m\n",
    "\n",
    "        return y_tripstart_pred, y_tripend_pred, y_distance_pred, y_purpouse_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 shape: torch.Size([14, 1, 57])\n",
      "\n",
      "Categorical outputs:  tensor([[-1.0620, -1.0620, -1.0620, -1.0620, -1.0620, -1.0620, -1.0620, -1.0620,\n",
      "         -1.0620, -1.0620, -1.0620, -1.0620, -1.0620, -1.0620, -1.0620, -1.0620,\n",
      "         -1.0620, -1.0620, -1.0620, -1.0620, -1.0620, -1.0620, -1.0620, -1.0620],\n",
      "        [-0.6914, -0.6914, -0.6914, -0.6914, -0.6914, -0.6914, -0.6914, -0.6914,\n",
      "         -0.6914, -0.6914, -0.6914, -0.6914, -0.6914, -0.6914, -0.6914, -0.6914,\n",
      "         -0.6914, -0.6914, -0.6914, -0.6914, -0.6914, -0.6914, -0.6914, -0.6914],\n",
      "        [ 0.2944,  0.2944,  0.2944,  0.2944,  0.2944,  0.2944,  0.2944,  0.2944,\n",
      "          0.2944,  0.2944,  0.2944,  0.2944,  0.2944,  0.2944,  0.2944,  0.2944,\n",
      "          0.2944,  0.2944,  0.2944,  0.2944,  0.2944,  0.2944,  0.2944,  0.2944],\n",
      "        [-0.1192, -0.1192, -0.1192, -0.1192, -0.1192, -0.1192, -0.1192, -0.1192,\n",
      "         -0.1192, -0.1192, -0.1192, -0.1192, -0.1192, -0.1192, -0.1192, -0.1192,\n",
      "         -0.1192, -0.1192, -0.1192, -0.1192, -0.1192, -0.1192, -0.1192, -0.1192],\n",
      "        [-0.2616, -0.2616, -0.2616, -0.2616, -0.2616, -0.2616, -0.2616, -0.2616,\n",
      "         -0.2616, -0.2616, -0.2616, -0.2616, -0.2616, -0.2616, -0.2616, -0.2616,\n",
      "         -0.2616, -0.2616, -0.2616, -0.2616, -0.2616, -0.2616, -0.2616, -0.2616],\n",
      "        [-0.3350, -0.3350, -0.3350, -0.3350, -0.3350, -0.3350, -0.3350, -0.3350,\n",
      "         -0.3350, -0.3350, -0.3350, -0.3350, -0.3350, -0.3350, -0.3350, -0.3350,\n",
      "         -0.3350, -0.3350, -0.3350, -0.3350, -0.3350, -0.3350, -0.3350, -0.3350],\n",
      "        [ 0.4601,  0.4601,  0.4601,  0.4601,  0.4601,  0.4601,  0.4601,  0.4601,\n",
      "          0.4601,  0.4601,  0.4601,  0.4601,  0.4601,  0.4601,  0.4601,  0.4601,\n",
      "          0.4601,  0.4601,  0.4601,  0.4601,  0.4601,  0.4601,  0.4601,  0.4601],\n",
      "        [ 0.1020,  0.1020,  0.1020,  0.1020,  0.1020,  0.1020,  0.1020,  0.1020,\n",
      "          0.1020,  0.1020,  0.1020,  0.1020,  0.1020,  0.1020,  0.1020,  0.1020,\n",
      "          0.1020,  0.1020,  0.1020,  0.1020,  0.1020,  0.1020,  0.1020,  0.1020],\n",
      "        [-0.6052, -0.6052, -0.6052, -0.6052, -0.6052, -0.6052, -0.6052, -0.6052,\n",
      "         -0.6052, -0.6052, -0.6052, -0.6052, -0.6052, -0.6052, -0.6052, -0.6052,\n",
      "         -0.6052, -0.6052, -0.6052, -0.6052, -0.6052, -0.6052, -0.6052, -0.6052],\n",
      "        [-0.3990, -0.3990, -0.3990, -0.3990, -0.3990, -0.3990, -0.3990, -0.3990,\n",
      "         -0.3990, -0.3990, -0.3990, -0.3990, -0.3990, -0.3990, -0.3990, -0.3990,\n",
      "         -0.3990, -0.3990, -0.3990, -0.3990, -0.3990, -0.3990, -0.3990, -0.3990]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Ground truth categorical: torch.Size([10])\n",
      "\n",
      "Categorical loss: 3.178054094314575\n",
      "Continous loss: 0.09094042330980301\n"
     ]
    }
   ],
   "source": [
    "# Taking one test draw\n",
    "\n",
    "rnn_model = RNNmodel()\n",
    "\n",
    "X0 = X[0,:,0,:].unsqueeze(1).to(torch.float32)\n",
    "print(f\"X1 shape: {X0.shape}\")\n",
    "print(\"\")\n",
    "\n",
    "y_tripstart_pred, y_tripend_pred, y_distance_pred, y_purpouse_pred = rnn_model.forward(X0)\n",
    "y_tripstart_true, y_tripend_true, y_distance_true, y_purpouse_true = y_cont[0,0,:10], y_cont[0,0,10:20], y_cont[0,0,20:], y_cat[0,0,:]\n",
    "\n",
    "y_cat_i = y_cat[0,0,:]\n",
    "\n",
    "print(f\"Categorical outputs:  {y_purpouse_pred}\")\n",
    "print(f\"Ground truth categorical: {y_purpouse_true.shape}\")\n",
    "print(\"\")\n",
    "#print(f\"Continous outputs:  {y_cont_hat}\")\n",
    "#print(f\"Ground truth Continous: {y_cont[0,0,:].shape}\")\n",
    "\n",
    "loss_cat = nn.CrossEntropyLoss()  #(y_hat, y)\n",
    "loss_cont = nn.MSELoss()\n",
    "\n",
    "print(f\"Categorical loss: {loss_cat(y_purpouse_pred, y_purpouse_true)}\")\n",
    "print(f\"Continous loss: {loss_cont(y_tripstart_pred, y_tripstart_true)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | individual: 0 | loss: 5.206408\n",
      "epoch: 0 | individual: 0 | loss: 5.582170\n",
      "epoch: 0 | individual: 0 | loss: 5.626668\n",
      "epoch: 0 | individual: 0 | loss: 5.766030\n",
      "epoch: 0 | individual: 0 | loss: 5.192911\n",
      "epoch: 0 | individual: 0 | loss: 4.811205\n",
      "epoch: 0 | individual: 1 | loss: 5.350414\n",
      "epoch: 0 | individual: 1 | loss: 5.164274\n",
      "epoch: 0 | individual: 1 | loss: 5.794714\n",
      "epoch: 0 | individual: 1 | loss: 6.136355\n",
      "epoch: 0 | individual: 1 | loss: 5.806646\n",
      "epoch: 0 | individual: 1 | loss: 4.964837\n",
      "epoch: 0 | individual: 2 | loss: 4.971531\n",
      "epoch: 0 | individual: 2 | loss: 4.923697\n",
      "epoch: 0 | individual: 2 | loss: 5.440995\n",
      "epoch: 0 | individual: 2 | loss: 5.820283\n",
      "epoch: 0 | individual: 2 | loss: 5.767117\n",
      "epoch: 0 | individual: 2 | loss: 4.867743\n",
      "epoch: 0 | individual: 3 | loss: 4.781367\n",
      "epoch: 0 | individual: 3 | loss: 4.818418\n",
      "epoch: 0 | individual: 3 | loss: 5.165185\n",
      "epoch: 0 | individual: 3 | loss: 5.579930\n",
      "epoch: 0 | individual: 3 | loss: 5.573334\n",
      "epoch: 0 | individual: 3 | loss: 4.739543\n",
      "epoch: 0 | individual: 4 | loss: 4.784124\n",
      "epoch: 0 | individual: 4 | loss: 4.752979\n",
      "epoch: 0 | individual: 4 | loss: 4.738211\n",
      "epoch: 0 | individual: 4 | loss: 5.134764\n",
      "epoch: 0 | individual: 4 | loss: 5.028913\n",
      "epoch: 0 | individual: 4 | loss: 4.644351\n",
      "epoch: 0 | individual: 5 | loss: 4.662583\n",
      "epoch: 0 | individual: 5 | loss: 4.321514\n",
      "epoch: 0 | individual: 5 | loss: 5.082171\n",
      "epoch: 0 | individual: 5 | loss: 4.542932\n",
      "epoch: 0 | individual: 5 | loss: 4.623713\n",
      "epoch: 0 | individual: 5 | loss: 4.589627\n",
      "epoch: 0 | individual: 6 | loss: 4.671403\n",
      "epoch: 0 | individual: 6 | loss: 4.343691\n",
      "epoch: 0 | individual: 6 | loss: 4.450488\n",
      "epoch: 0 | individual: 6 | loss: 4.505755\n",
      "epoch: 0 | individual: 6 | loss: 4.562293\n",
      "epoch: 0 | individual: 6 | loss: 4.594881\n",
      "epoch: 0 | individual: 7 | loss: 4.580271\n",
      "epoch: 0 | individual: 7 | loss: 4.174741\n",
      "epoch: 0 | individual: 7 | loss: 4.526949\n",
      "epoch: 0 | individual: 7 | loss: 4.330911\n",
      "epoch: 0 | individual: 7 | loss: 4.566621\n",
      "epoch: 0 | individual: 7 | loss: 4.379303\n",
      "epoch: 0 | individual: 8 | loss: 4.342000\n",
      "epoch: 0 | individual: 8 | loss: 4.283457\n",
      "epoch: 0 | individual: 8 | loss: 4.300798\n",
      "epoch: 0 | individual: 8 | loss: 4.330075\n",
      "epoch: 0 | individual: 8 | loss: 4.346242\n",
      "epoch: 0 | individual: 8 | loss: 4.321941\n",
      "epoch: 0 | individual: 9 | loss: 4.210098\n",
      "epoch: 0 | individual: 9 | loss: 4.068161\n",
      "epoch: 0 | individual: 9 | loss: 4.372917\n",
      "epoch: 0 | individual: 9 | loss: 4.530250\n",
      "epoch: 0 | individual: 9 | loss: 5.987862\n",
      "epoch: 0 | individual: 9 | loss: 4.329389\n",
      "epoch: 0 | individual: 10 | loss: 4.134006\n",
      "epoch: 0 | individual: 10 | loss: 4.082663\n",
      "epoch: 0 | individual: 10 | loss: 4.295695\n",
      "epoch: 0 | individual: 10 | loss: 4.662465\n",
      "epoch: 0 | individual: 10 | loss: 4.286192\n",
      "epoch: 0 | individual: 10 | loss: 4.250515\n",
      "epoch: 0 | individual: 11 | loss: 4.050257\n",
      "epoch: 0 | individual: 11 | loss: 4.206156\n",
      "epoch: 0 | individual: 11 | loss: 4.240851\n",
      "epoch: 0 | individual: 11 | loss: 4.104676\n",
      "epoch: 0 | individual: 11 | loss: 4.173699\n",
      "epoch: 0 | individual: 11 | loss: 4.125349\n",
      "epoch: 0 | individual: 12 | loss: 4.171031\n",
      "epoch: 0 | individual: 12 | loss: 3.930549\n",
      "epoch: 0 | individual: 12 | loss: 4.089669\n",
      "epoch: 0 | individual: 12 | loss: 4.121076\n",
      "epoch: 0 | individual: 12 | loss: 4.212640\n",
      "epoch: 0 | individual: 12 | loss: 4.550917\n",
      "epoch: 0 | individual: 13 | loss: 4.092205\n",
      "epoch: 0 | individual: 13 | loss: 3.956337\n",
      "epoch: 0 | individual: 13 | loss: 4.029851\n",
      "epoch: 0 | individual: 13 | loss: 4.057079\n",
      "epoch: 0 | individual: 13 | loss: 4.267339\n",
      "epoch: 0 | individual: 13 | loss: 3.998862\n",
      "epoch: 0 | individual: 14 | loss: 3.794239\n",
      "epoch: 0 | individual: 14 | loss: 3.995780\n",
      "epoch: 0 | individual: 14 | loss: 4.024856\n",
      "epoch: 0 | individual: 14 | loss: 4.071688\n",
      "epoch: 0 | individual: 14 | loss: 3.936767\n",
      "epoch: 0 | individual: 14 | loss: 4.006510\n",
      "epoch: 0 | individual: 15 | loss: 3.908403\n",
      "epoch: 0 | individual: 15 | loss: 3.855303\n",
      "epoch: 0 | individual: 15 | loss: 3.828454\n",
      "epoch: 0 | individual: 15 | loss: 3.994349\n",
      "epoch: 0 | individual: 15 | loss: 3.948246\n",
      "epoch: 0 | individual: 15 | loss: 3.792655\n",
      "epoch: 0 | individual: 16 | loss: 3.861511\n",
      "epoch: 0 | individual: 16 | loss: 4.071944\n",
      "epoch: 0 | individual: 16 | loss: 6.983874\n",
      "epoch: 0 | individual: 16 | loss: 3.792643\n",
      "epoch: 0 | individual: 16 | loss: 3.852260\n",
      "epoch: 0 | individual: 16 | loss: 3.915982\n",
      "epoch: 0 | individual: 17 | loss: 3.885845\n",
      "epoch: 0 | individual: 17 | loss: 3.879507\n",
      "epoch: 0 | individual: 17 | loss: 3.894749\n",
      "epoch: 0 | individual: 17 | loss: 4.199694\n",
      "epoch: 0 | individual: 17 | loss: 4.846498\n",
      "epoch: 0 | individual: 17 | loss: 4.168699\n",
      "epoch: 0 | individual: 18 | loss: 3.927534\n",
      "epoch: 0 | individual: 18 | loss: 3.781408\n",
      "epoch: 0 | individual: 18 | loss: 3.930786\n",
      "epoch: 0 | individual: 18 | loss: 3.796824\n",
      "epoch: 0 | individual: 18 | loss: 3.839942\n",
      "epoch: 0 | individual: 18 | loss: 3.813700\n",
      "epoch: 0 | individual: 19 | loss: 4.110664\n",
      "epoch: 0 | individual: 19 | loss: 3.790916\n",
      "epoch: 0 | individual: 19 | loss: 3.857081\n",
      "epoch: 0 | individual: 19 | loss: 3.982587\n",
      "epoch: 0 | individual: 19 | loss: 3.829127\n",
      "epoch: 0 | individual: 19 | loss: 3.991464\n",
      "epoch: 0 | individual: 20 | loss: 3.965820\n",
      "epoch: 0 | individual: 20 | loss: 5.330554\n",
      "epoch: 0 | individual: 20 | loss: 4.905502\n",
      "epoch: 0 | individual: 20 | loss: 4.282020\n",
      "epoch: 0 | individual: 20 | loss: 4.044424\n",
      "epoch: 0 | individual: 20 | loss: 3.894459\n",
      "epoch: 0 | individual: 21 | loss: 3.789147\n",
      "epoch: 0 | individual: 21 | loss: 3.743776\n",
      "epoch: 0 | individual: 21 | loss: 3.760818\n",
      "epoch: 0 | individual: 21 | loss: 3.833269\n",
      "epoch: 0 | individual: 21 | loss: 3.674871\n",
      "epoch: 0 | individual: 21 | loss: 3.750778\n",
      "epoch: 0 | individual: 22 | loss: 3.765836\n",
      "epoch: 0 | individual: 22 | loss: 3.661451\n",
      "epoch: 0 | individual: 22 | loss: 3.778010\n",
      "epoch: 0 | individual: 22 | loss: 3.777817\n",
      "epoch: 0 | individual: 22 | loss: 3.757703\n",
      "epoch: 0 | individual: 22 | loss: 3.739912\n",
      "epoch: 0 | individual: 23 | loss: 3.776854\n",
      "epoch: 0 | individual: 23 | loss: 3.727284\n",
      "epoch: 0 | individual: 23 | loss: 3.729432\n",
      "epoch: 0 | individual: 23 | loss: 3.650599\n",
      "epoch: 0 | individual: 23 | loss: 3.671495\n",
      "epoch: 0 | individual: 23 | loss: 3.720863\n",
      "epoch: 0 | individual: 24 | loss: 3.660207\n",
      "epoch: 0 | individual: 24 | loss: 3.773092\n",
      "epoch: 0 | individual: 24 | loss: 3.796940\n",
      "epoch: 0 | individual: 24 | loss: 3.719691\n",
      "epoch: 0 | individual: 24 | loss: 3.675423\n",
      "epoch: 0 | individual: 24 | loss: 3.778809\n",
      "epoch: 0 | individual: 25 | loss: 3.613279\n",
      "epoch: 0 | individual: 25 | loss: 3.619663\n",
      "epoch: 0 | individual: 25 | loss: 3.633328\n",
      "epoch: 0 | individual: 25 | loss: 3.635980\n",
      "epoch: 0 | individual: 25 | loss: 3.889892\n",
      "epoch: 0 | individual: 25 | loss: 3.643989\n",
      "epoch: 0 | individual: 26 | loss: 3.608229\n",
      "epoch: 0 | individual: 26 | loss: 3.625834\n",
      "epoch: 0 | individual: 26 | loss: 3.670625\n",
      "epoch: 0 | individual: 26 | loss: 3.694079\n",
      "epoch: 0 | individual: 26 | loss: 3.534147\n",
      "epoch: 0 | individual: 26 | loss: 3.591846\n",
      "epoch: 0 | individual: 27 | loss: 3.543781\n",
      "epoch: 0 | individual: 27 | loss: 3.619679\n",
      "epoch: 0 | individual: 27 | loss: 3.704051\n",
      "epoch: 0 | individual: 27 | loss: 3.620642\n",
      "epoch: 0 | individual: 27 | loss: 3.670787\n",
      "epoch: 0 | individual: 27 | loss: 3.621920\n",
      "epoch: 0 | individual: 28 | loss: 3.566513\n",
      "epoch: 0 | individual: 28 | loss: 3.617091\n",
      "epoch: 0 | individual: 28 | loss: 3.627908\n",
      "epoch: 0 | individual: 28 | loss: 3.577574\n",
      "epoch: 0 | individual: 28 | loss: 3.603691\n",
      "epoch: 0 | individual: 28 | loss: 3.698513\n",
      "epoch: 0 | individual: 29 | loss: 3.553329\n",
      "epoch: 0 | individual: 29 | loss: 3.588155\n",
      "epoch: 0 | individual: 29 | loss: 3.592906\n",
      "epoch: 0 | individual: 29 | loss: 3.624879\n",
      "epoch: 0 | individual: 29 | loss: 3.716189\n",
      "epoch: 0 | individual: 29 | loss: 4.468596\n",
      "epoch: 0 | individual: 30 | loss: 4.704852\n",
      "epoch: 0 | individual: 30 | loss: 5.187499\n",
      "epoch: 0 | individual: 30 | loss: 4.964002\n",
      "epoch: 0 | individual: 30 | loss: 3.770947\n",
      "epoch: 0 | individual: 30 | loss: 3.514004\n",
      "epoch: 0 | individual: 30 | loss: 3.605243\n",
      "epoch: 0 | individual: 31 | loss: 3.603603\n",
      "epoch: 0 | individual: 31 | loss: 3.534704\n",
      "epoch: 0 | individual: 31 | loss: 3.549744\n",
      "epoch: 0 | individual: 31 | loss: 3.802932\n",
      "epoch: 0 | individual: 31 | loss: 4.083789\n",
      "epoch: 0 | individual: 31 | loss: 4.001918\n",
      "epoch: 0 | individual: 32 | loss: 3.513840\n",
      "epoch: 0 | individual: 32 | loss: 3.607576\n",
      "epoch: 0 | individual: 32 | loss: 3.566394\n",
      "epoch: 0 | individual: 32 | loss: 3.527831\n",
      "epoch: 0 | individual: 32 | loss: 3.599357\n",
      "epoch: 0 | individual: 32 | loss: 3.591637\n",
      "epoch: 0 | individual: 33 | loss: 3.604434\n",
      "epoch: 0 | individual: 33 | loss: 3.896664\n",
      "epoch: 0 | individual: 33 | loss: 3.576526\n",
      "epoch: 0 | individual: 33 | loss: 3.690559\n",
      "epoch: 0 | individual: 33 | loss: 3.518243\n",
      "epoch: 0 | individual: 33 | loss: 3.635114\n",
      "epoch: 0 | individual: 34 | loss: 3.550221\n",
      "epoch: 0 | individual: 34 | loss: 3.694861\n",
      "epoch: 0 | individual: 34 | loss: 4.001734\n",
      "epoch: 0 | individual: 34 | loss: 3.568874\n",
      "epoch: 0 | individual: 34 | loss: 3.583903\n",
      "epoch: 0 | individual: 34 | loss: 3.510751\n",
      "epoch: 0 | individual: 35 | loss: 3.689818\n",
      "epoch: 0 | individual: 35 | loss: 3.665100\n",
      "epoch: 0 | individual: 35 | loss: 3.546520\n",
      "epoch: 0 | individual: 35 | loss: 3.692958\n",
      "epoch: 0 | individual: 35 | loss: 3.562428\n",
      "epoch: 0 | individual: 35 | loss: 3.675858\n",
      "epoch: 0 | individual: 36 | loss: 3.530406\n",
      "epoch: 0 | individual: 36 | loss: 4.331286\n",
      "epoch: 0 | individual: 36 | loss: 4.905957\n",
      "epoch: 0 | individual: 36 | loss: 3.514442\n",
      "epoch: 0 | individual: 36 | loss: 3.583278\n",
      "epoch: 0 | individual: 36 | loss: 3.690615\n",
      "epoch: 0 | individual: 37 | loss: 3.851809\n",
      "epoch: 0 | individual: 37 | loss: 4.463722\n",
      "epoch: 0 | individual: 37 | loss: 4.806395\n",
      "epoch: 0 | individual: 37 | loss: 4.422553\n",
      "epoch: 0 | individual: 37 | loss: 4.511091\n",
      "epoch: 0 | individual: 37 | loss: 3.573441\n",
      "epoch: 0 | individual: 38 | loss: 3.671808\n",
      "epoch: 0 | individual: 38 | loss: 4.181543\n",
      "epoch: 0 | individual: 38 | loss: 3.577649\n",
      "epoch: 0 | individual: 38 | loss: 4.035601\n",
      "epoch: 0 | individual: 38 | loss: 3.559654\n",
      "epoch: 0 | individual: 38 | loss: 3.601108\n",
      "epoch: 0 | individual: 39 | loss: 3.687951\n",
      "epoch: 0 | individual: 39 | loss: 3.539263\n",
      "epoch: 0 | individual: 39 | loss: 3.608239\n",
      "epoch: 0 | individual: 39 | loss: 3.779169\n",
      "epoch: 0 | individual: 39 | loss: 3.749999\n",
      "epoch: 0 | individual: 39 | loss: 3.564542\n",
      "epoch: 0 | individual: 40 | loss: 3.616299\n",
      "epoch: 0 | individual: 40 | loss: 3.578291\n",
      "epoch: 0 | individual: 40 | loss: 3.725454\n",
      "epoch: 0 | individual: 40 | loss: 3.615755\n",
      "epoch: 0 | individual: 40 | loss: 3.631673\n",
      "epoch: 0 | individual: 40 | loss: 3.592883\n",
      "epoch: 0 | individual: 41 | loss: 3.505998\n",
      "epoch: 0 | individual: 41 | loss: 3.534375\n",
      "epoch: 0 | individual: 41 | loss: 3.764730\n",
      "epoch: 0 | individual: 41 | loss: 3.506908\n",
      "epoch: 0 | individual: 41 | loss: 3.596606\n",
      "epoch: 0 | individual: 41 | loss: 3.553156\n",
      "epoch: 0 | individual: 42 | loss: 3.575158\n",
      "epoch: 0 | individual: 42 | loss: 3.549696\n",
      "epoch: 0 | individual: 42 | loss: 3.978212\n",
      "epoch: 0 | individual: 42 | loss: 5.224264\n",
      "epoch: 0 | individual: 42 | loss: 5.157013\n",
      "epoch: 0 | individual: 42 | loss: 4.741921\n",
      "epoch: 0 | individual: 43 | loss: 3.513481\n",
      "epoch: 0 | individual: 43 | loss: 3.508133\n",
      "epoch: 0 | individual: 43 | loss: 3.530599\n",
      "epoch: 0 | individual: 43 | loss: 3.526014\n",
      "epoch: 0 | individual: 43 | loss: 3.564710\n",
      "epoch: 0 | individual: 43 | loss: 3.529453\n",
      "epoch: 0 | individual: 44 | loss: 3.522577\n",
      "epoch: 0 | individual: 44 | loss: 3.553437\n",
      "epoch: 0 | individual: 44 | loss: 3.599831\n",
      "epoch: 0 | individual: 44 | loss: 4.765725\n",
      "epoch: 0 | individual: 44 | loss: 3.567166\n",
      "epoch: 0 | individual: 44 | loss: 3.577581\n",
      "epoch: 0 | individual: 45 | loss: 3.464252\n",
      "epoch: 0 | individual: 45 | loss: 3.557128\n",
      "epoch: 0 | individual: 45 | loss: 3.489019\n",
      "epoch: 0 | individual: 45 | loss: 3.541797\n",
      "epoch: 0 | individual: 45 | loss: 3.462122\n",
      "epoch: 0 | individual: 45 | loss: 3.512174\n",
      "epoch: 0 | individual: 46 | loss: 3.840199\n",
      "epoch: 0 | individual: 46 | loss: 3.678329\n",
      "epoch: 0 | individual: 46 | loss: 3.773830\n",
      "epoch: 0 | individual: 46 | loss: 3.594981\n",
      "epoch: 0 | individual: 46 | loss: 3.579094\n",
      "epoch: 0 | individual: 46 | loss: 3.529441\n",
      "epoch: 0 | individual: 47 | loss: 3.502815\n",
      "epoch: 0 | individual: 47 | loss: 3.553792\n",
      "epoch: 0 | individual: 47 | loss: 3.828450\n",
      "epoch: 0 | individual: 47 | loss: 3.551937\n",
      "epoch: 0 | individual: 47 | loss: 3.735905\n",
      "epoch: 0 | individual: 47 | loss: 3.541002\n",
      "epoch: 0 | individual: 48 | loss: 3.785099\n",
      "epoch: 0 | individual: 48 | loss: 5.045494\n",
      "epoch: 0 | individual: 48 | loss: 3.368516\n",
      "epoch: 0 | individual: 48 | loss: 3.463915\n",
      "epoch: 0 | individual: 48 | loss: 3.575482\n",
      "epoch: 0 | individual: 48 | loss: 3.574921\n",
      "epoch: 0 | individual: 49 | loss: 3.679334\n",
      "epoch: 0 | individual: 49 | loss: 3.770353\n",
      "epoch: 0 | individual: 49 | loss: 3.776573\n",
      "epoch: 0 | individual: 49 | loss: 3.542688\n",
      "epoch: 0 | individual: 49 | loss: 3.650567\n",
      "epoch: 0 | individual: 49 | loss: 3.682351\n",
      "epoch: 0 | individual: 50 | loss: 3.532933\n",
      "epoch: 0 | individual: 50 | loss: 3.524645\n",
      "epoch: 0 | individual: 50 | loss: 3.549744\n",
      "epoch: 0 | individual: 50 | loss: 3.693828\n",
      "epoch: 0 | individual: 50 | loss: 3.498700\n",
      "epoch: 0 | individual: 50 | loss: 3.538243\n",
      "epoch: 0 | individual: 51 | loss: 3.744970\n",
      "epoch: 0 | individual: 51 | loss: 4.861769\n",
      "epoch: 0 | individual: 51 | loss: 3.759354\n",
      "epoch: 0 | individual: 51 | loss: 3.546870\n",
      "epoch: 0 | individual: 51 | loss: 3.439712\n",
      "epoch: 0 | individual: 51 | loss: 3.491577\n",
      "epoch: 0 | individual: 52 | loss: 3.480578\n",
      "epoch: 0 | individual: 52 | loss: 3.540434\n",
      "epoch: 0 | individual: 52 | loss: 3.488749\n",
      "epoch: 0 | individual: 52 | loss: 3.488369\n",
      "epoch: 0 | individual: 52 | loss: 3.482421\n",
      "epoch: 0 | individual: 52 | loss: 3.537115\n",
      "epoch: 0 | individual: 53 | loss: 3.584987\n",
      "epoch: 0 | individual: 53 | loss: 3.590469\n",
      "epoch: 0 | individual: 53 | loss: 3.845885\n",
      "epoch: 0 | individual: 53 | loss: 4.610520\n",
      "epoch: 0 | individual: 53 | loss: 3.801242\n",
      "epoch: 0 | individual: 53 | loss: 4.755296\n",
      "epoch: 0 | individual: 54 | loss: 3.521992\n",
      "epoch: 0 | individual: 54 | loss: 3.476015\n",
      "epoch: 0 | individual: 54 | loss: 3.494271\n",
      "epoch: 0 | individual: 54 | loss: 3.495934\n",
      "epoch: 0 | individual: 54 | loss: 3.454898\n",
      "epoch: 0 | individual: 54 | loss: 3.505461\n",
      "epoch: 0 | individual: 55 | loss: 3.607135\n",
      "epoch: 0 | individual: 55 | loss: 3.477337\n",
      "epoch: 0 | individual: 55 | loss: 3.482184\n",
      "epoch: 0 | individual: 55 | loss: 3.638300\n",
      "epoch: 0 | individual: 55 | loss: 3.472313\n",
      "epoch: 0 | individual: 55 | loss: 3.509780\n",
      "epoch: 0 | individual: 56 | loss: 3.510730\n",
      "epoch: 0 | individual: 56 | loss: 3.500957\n",
      "epoch: 0 | individual: 56 | loss: 3.620454\n",
      "epoch: 0 | individual: 56 | loss: 4.020753\n",
      "epoch: 0 | individual: 56 | loss: 3.493614\n",
      "epoch: 0 | individual: 56 | loss: 3.501447\n",
      "epoch: 0 | individual: 57 | loss: 3.458916\n",
      "epoch: 0 | individual: 57 | loss: 3.530899\n",
      "epoch: 0 | individual: 57 | loss: 3.535719\n",
      "epoch: 0 | individual: 57 | loss: 3.503683\n",
      "epoch: 0 | individual: 57 | loss: 3.530478\n",
      "epoch: 0 | individual: 57 | loss: 3.502484\n",
      "epoch: 0 | individual: 58 | loss: 4.735838\n",
      "epoch: 0 | individual: 58 | loss: 4.746160\n",
      "epoch: 0 | individual: 58 | loss: 4.736807\n",
      "epoch: 0 | individual: 58 | loss: 4.738440\n",
      "epoch: 0 | individual: 58 | loss: 4.484754\n",
      "epoch: 0 | individual: 58 | loss: 3.651359\n",
      "epoch: 0 | individual: 59 | loss: 3.513608\n",
      "epoch: 0 | individual: 59 | loss: 3.465931\n",
      "epoch: 0 | individual: 59 | loss: 3.468598\n",
      "epoch: 0 | individual: 59 | loss: 3.473956\n",
      "epoch: 0 | individual: 59 | loss: 3.519987\n",
      "epoch: 0 | individual: 59 | loss: 3.501866\n",
      "epoch: 0 | individual: 60 | loss: 3.480489\n",
      "epoch: 0 | individual: 60 | loss: 3.487187\n",
      "epoch: 0 | individual: 60 | loss: 3.506332\n",
      "epoch: 0 | individual: 60 | loss: 3.594023\n",
      "epoch: 0 | individual: 60 | loss: 3.754855\n",
      "epoch: 0 | individual: 60 | loss: 3.550814\n",
      "epoch: 0 | individual: 61 | loss: 3.472396\n",
      "epoch: 0 | individual: 61 | loss: 3.470777\n",
      "epoch: 0 | individual: 61 | loss: 3.447528\n",
      "epoch: 0 | individual: 61 | loss: 3.526090\n",
      "epoch: 0 | individual: 61 | loss: 3.544767\n",
      "epoch: 0 | individual: 61 | loss: 3.529414\n",
      "epoch: 0 | individual: 62 | loss: 3.519895\n",
      "epoch: 0 | individual: 62 | loss: 3.442260\n",
      "epoch: 0 | individual: 62 | loss: 3.506191\n",
      "epoch: 0 | individual: 62 | loss: 3.894429\n",
      "epoch: 0 | individual: 62 | loss: 3.512504\n",
      "epoch: 0 | individual: 62 | loss: 3.544143\n",
      "epoch: 0 | individual: 63 | loss: 3.519643\n",
      "epoch: 0 | individual: 63 | loss: 3.687678\n",
      "epoch: 0 | individual: 63 | loss: 3.471688\n",
      "epoch: 0 | individual: 63 | loss: 3.495327\n",
      "epoch: 0 | individual: 63 | loss: 3.431026\n",
      "epoch: 0 | individual: 63 | loss: 3.467203\n",
      "epoch: 0 | individual: 64 | loss: 3.476227\n",
      "epoch: 0 | individual: 64 | loss: 3.484362\n",
      "epoch: 0 | individual: 64 | loss: 3.596360\n",
      "epoch: 0 | individual: 64 | loss: 4.299815\n",
      "epoch: 0 | individual: 64 | loss: 3.522800\n",
      "epoch: 0 | individual: 64 | loss: 3.473665\n",
      "epoch: 0 | individual: 65 | loss: 3.522409\n",
      "epoch: 0 | individual: 65 | loss: 3.398402\n",
      "epoch: 0 | individual: 65 | loss: 3.466859\n",
      "epoch: 0 | individual: 65 | loss: 3.971849\n",
      "epoch: 0 | individual: 65 | loss: 3.463395\n",
      "epoch: 0 | individual: 65 | loss: 3.520301\n",
      "epoch: 0 | individual: 66 | loss: 3.478629\n",
      "epoch: 0 | individual: 66 | loss: 3.519099\n",
      "epoch: 0 | individual: 66 | loss: 3.423417\n",
      "epoch: 0 | individual: 66 | loss: 3.948386\n",
      "epoch: 0 | individual: 66 | loss: 4.403037\n",
      "epoch: 0 | individual: 66 | loss: 4.100368\n",
      "epoch: 0 | individual: 67 | loss: 3.650816\n",
      "epoch: 0 | individual: 67 | loss: 3.431703\n",
      "epoch: 0 | individual: 67 | loss: 3.460668\n",
      "epoch: 0 | individual: 67 | loss: 3.442922\n",
      "epoch: 0 | individual: 67 | loss: 3.516560\n",
      "epoch: 0 | individual: 67 | loss: 3.448937\n",
      "epoch: 0 | individual: 68 | loss: 3.571253\n",
      "epoch: 0 | individual: 68 | loss: 3.464304\n",
      "epoch: 0 | individual: 68 | loss: 3.514415\n",
      "epoch: 0 | individual: 68 | loss: 3.577705\n",
      "epoch: 0 | individual: 68 | loss: 3.483105\n",
      "epoch: 0 | individual: 68 | loss: 3.592381\n",
      "epoch: 0 | individual: 69 | loss: 3.441675\n",
      "epoch: 0 | individual: 69 | loss: 3.505516\n",
      "epoch: 0 | individual: 69 | loss: 3.385924\n",
      "epoch: 0 | individual: 69 | loss: 3.472236\n",
      "epoch: 0 | individual: 69 | loss: 3.505269\n",
      "epoch: 0 | individual: 69 | loss: 3.460995\n",
      "epoch: 0 | individual: 70 | loss: 3.458967\n",
      "epoch: 0 | individual: 70 | loss: 3.432152\n",
      "epoch: 0 | individual: 70 | loss: 3.387610\n",
      "epoch: 0 | individual: 70 | loss: 3.357269\n",
      "epoch: 0 | individual: 70 | loss: 3.524413\n",
      "epoch: 0 | individual: 70 | loss: 3.497155\n",
      "epoch: 0 | individual: 71 | loss: 3.771060\n",
      "epoch: 0 | individual: 71 | loss: 3.452195\n",
      "epoch: 0 | individual: 71 | loss: 3.468047\n",
      "epoch: 0 | individual: 71 | loss: 3.455995\n",
      "epoch: 0 | individual: 71 | loss: 3.452055\n",
      "epoch: 0 | individual: 71 | loss: 3.470758\n",
      "epoch: 0 | individual: 72 | loss: 3.425867\n",
      "epoch: 0 | individual: 72 | loss: 3.482759\n",
      "epoch: 0 | individual: 72 | loss: 3.460136\n",
      "epoch: 0 | individual: 72 | loss: 3.458185\n",
      "epoch: 0 | individual: 72 | loss: 3.502986\n",
      "epoch: 0 | individual: 72 | loss: 3.452164\n",
      "epoch: 0 | individual: 73 | loss: 3.518050\n",
      "epoch: 0 | individual: 73 | loss: 3.463039\n",
      "epoch: 0 | individual: 73 | loss: 3.362196\n",
      "epoch: 0 | individual: 73 | loss: 3.490318\n",
      "epoch: 0 | individual: 73 | loss: 3.499369\n",
      "epoch: 0 | individual: 73 | loss: 3.494713\n",
      "epoch: 0 | individual: 74 | loss: 3.470670\n",
      "epoch: 0 | individual: 74 | loss: 3.451768\n",
      "epoch: 0 | individual: 74 | loss: 3.471207\n",
      "epoch: 0 | individual: 74 | loss: 3.438259\n",
      "epoch: 0 | individual: 74 | loss: 3.445176\n",
      "epoch: 0 | individual: 74 | loss: 3.419803\n",
      "epoch: 0 | individual: 75 | loss: 3.442378\n",
      "epoch: 0 | individual: 75 | loss: 3.455103\n",
      "epoch: 0 | individual: 75 | loss: 3.430888\n",
      "epoch: 0 | individual: 75 | loss: 3.404264\n",
      "epoch: 0 | individual: 75 | loss: 3.480340\n",
      "epoch: 0 | individual: 75 | loss: 3.401385\n",
      "epoch: 0 | individual: 76 | loss: 3.422762\n",
      "epoch: 0 | individual: 76 | loss: 3.433215\n",
      "epoch: 0 | individual: 76 | loss: 3.448816\n",
      "epoch: 0 | individual: 76 | loss: 3.697440\n",
      "epoch: 0 | individual: 76 | loss: 3.429460\n",
      "epoch: 0 | individual: 76 | loss: 3.501332\n",
      "epoch: 0 | individual: 77 | loss: 3.438789\n",
      "epoch: 0 | individual: 77 | loss: 3.436620\n",
      "epoch: 0 | individual: 77 | loss: 3.434968\n",
      "epoch: 0 | individual: 77 | loss: 3.433970\n",
      "epoch: 0 | individual: 77 | loss: 3.617536\n",
      "epoch: 0 | individual: 77 | loss: 3.421133\n",
      "epoch: 0 | individual: 78 | loss: 3.655541\n",
      "epoch: 0 | individual: 78 | loss: 3.489397\n",
      "epoch: 0 | individual: 78 | loss: 3.494679\n",
      "epoch: 0 | individual: 78 | loss: 3.534884\n",
      "epoch: 0 | individual: 78 | loss: 3.510694\n",
      "epoch: 0 | individual: 78 | loss: 3.480991\n",
      "epoch: 0 | individual: 79 | loss: 3.465249\n",
      "epoch: 0 | individual: 79 | loss: 3.457460\n",
      "epoch: 0 | individual: 79 | loss: 3.376611\n",
      "epoch: 0 | individual: 79 | loss: 3.435931\n",
      "epoch: 0 | individual: 79 | loss: 3.393084\n",
      "epoch: 0 | individual: 79 | loss: 3.443732\n",
      "epoch: 0 | individual: 80 | loss: 3.390267\n",
      "epoch: 0 | individual: 80 | loss: 3.388344\n",
      "epoch: 0 | individual: 80 | loss: 3.379071\n",
      "epoch: 0 | individual: 80 | loss: 3.414671\n",
      "epoch: 0 | individual: 80 | loss: 3.430244\n",
      "epoch: 0 | individual: 80 | loss: 3.403569\n",
      "epoch: 0 | individual: 81 | loss: 3.449947\n",
      "epoch: 0 | individual: 81 | loss: 3.495334\n",
      "epoch: 0 | individual: 81 | loss: 3.598428\n",
      "epoch: 0 | individual: 81 | loss: 3.544585\n",
      "epoch: 0 | individual: 81 | loss: 3.444850\n",
      "epoch: 0 | individual: 81 | loss: 3.389851\n",
      "epoch: 0 | individual: 82 | loss: 3.445481\n",
      "epoch: 0 | individual: 82 | loss: 3.441425\n",
      "epoch: 0 | individual: 82 | loss: 3.398507\n",
      "epoch: 0 | individual: 82 | loss: 3.471059\n",
      "epoch: 0 | individual: 82 | loss: 3.438803\n",
      "epoch: 0 | individual: 82 | loss: 3.379833\n",
      "epoch: 0 | individual: 83 | loss: 3.400717\n",
      "epoch: 0 | individual: 83 | loss: 3.421096\n",
      "epoch: 0 | individual: 83 | loss: 3.404567\n",
      "epoch: 0 | individual: 83 | loss: 3.396775\n",
      "epoch: 0 | individual: 83 | loss: 3.358867\n",
      "epoch: 0 | individual: 83 | loss: 3.397612\n",
      "epoch: 0 | individual: 84 | loss: 3.426456\n",
      "epoch: 0 | individual: 84 | loss: 3.425650\n",
      "epoch: 0 | individual: 84 | loss: 3.424395\n",
      "epoch: 0 | individual: 84 | loss: 3.422982\n",
      "epoch: 0 | individual: 84 | loss: 3.393411\n",
      "epoch: 0 | individual: 84 | loss: 3.401907\n",
      "epoch: 0 | individual: 85 | loss: 3.375555\n",
      "epoch: 0 | individual: 85 | loss: 3.375560\n",
      "epoch: 0 | individual: 85 | loss: 3.366668\n",
      "epoch: 0 | individual: 85 | loss: 3.359149\n",
      "epoch: 0 | individual: 85 | loss: 3.384438\n",
      "epoch: 0 | individual: 85 | loss: 3.405913\n",
      "epoch: 0 | individual: 86 | loss: 3.779764\n",
      "epoch: 0 | individual: 86 | loss: 4.368112\n",
      "epoch: 0 | individual: 86 | loss: 3.458312\n",
      "epoch: 0 | individual: 86 | loss: 3.659757\n",
      "epoch: 0 | individual: 86 | loss: 3.697163\n",
      "epoch: 0 | individual: 86 | loss: 3.627377\n",
      "epoch: 0 | individual: 87 | loss: 3.394991\n",
      "epoch: 0 | individual: 87 | loss: 3.546683\n",
      "epoch: 0 | individual: 87 | loss: 3.354350\n",
      "epoch: 0 | individual: 87 | loss: 3.351151\n",
      "epoch: 0 | individual: 87 | loss: 3.471228\n",
      "epoch: 0 | individual: 87 | loss: 3.345289\n",
      "epoch: 0 | individual: 88 | loss: 3.378716\n",
      "epoch: 0 | individual: 88 | loss: 3.463294\n",
      "epoch: 0 | individual: 88 | loss: 3.460569\n",
      "epoch: 0 | individual: 88 | loss: 3.645135\n",
      "epoch: 0 | individual: 88 | loss: 3.389908\n",
      "epoch: 0 | individual: 88 | loss: 3.411091\n",
      "epoch: 0 | individual: 89 | loss: 3.410327\n",
      "epoch: 0 | individual: 89 | loss: 3.560705\n",
      "epoch: 0 | individual: 89 | loss: 3.408795\n",
      "epoch: 0 | individual: 89 | loss: 3.554479\n",
      "epoch: 0 | individual: 89 | loss: 3.347159\n",
      "epoch: 0 | individual: 89 | loss: 3.343470\n",
      "epoch: 0 | individual: 90 | loss: 3.520520\n",
      "epoch: 0 | individual: 90 | loss: 3.350512\n",
      "epoch: 0 | individual: 90 | loss: 3.453161\n",
      "epoch: 0 | individual: 90 | loss: 3.402343\n",
      "epoch: 0 | individual: 90 | loss: 3.365582\n",
      "epoch: 0 | individual: 90 | loss: 3.370861\n",
      "epoch: 0 | individual: 91 | loss: 3.405496\n",
      "epoch: 0 | individual: 91 | loss: 3.355107\n",
      "epoch: 0 | individual: 91 | loss: 3.340502\n",
      "epoch: 0 | individual: 91 | loss: 3.406514\n",
      "epoch: 0 | individual: 91 | loss: 3.385412\n",
      "epoch: 0 | individual: 91 | loss: 3.394425\n",
      "epoch: 0 | individual: 92 | loss: 5.728060\n",
      "epoch: 0 | individual: 92 | loss: 5.216826\n",
      "epoch: 0 | individual: 92 | loss: 3.662481\n",
      "epoch: 0 | individual: 92 | loss: 3.331645\n",
      "epoch: 0 | individual: 92 | loss: 3.355933\n",
      "epoch: 0 | individual: 92 | loss: 3.376329\n",
      "epoch: 0 | individual: 93 | loss: 3.338786\n",
      "epoch: 0 | individual: 93 | loss: 3.337321\n",
      "epoch: 0 | individual: 93 | loss: 3.367511\n",
      "epoch: 0 | individual: 93 | loss: 3.316276\n",
      "epoch: 0 | individual: 93 | loss: 3.333326\n",
      "epoch: 0 | individual: 93 | loss: 3.384792\n",
      "epoch: 0 | individual: 94 | loss: 3.328713\n",
      "epoch: 0 | individual: 94 | loss: 3.357941\n",
      "epoch: 0 | individual: 94 | loss: 3.354775\n",
      "epoch: 0 | individual: 94 | loss: 3.335507\n",
      "epoch: 0 | individual: 94 | loss: 3.468269\n",
      "epoch: 0 | individual: 94 | loss: 3.343009\n",
      "epoch: 0 | individual: 95 | loss: 3.352051\n",
      "epoch: 0 | individual: 95 | loss: 3.351554\n",
      "epoch: 0 | individual: 95 | loss: 3.350966\n",
      "epoch: 0 | individual: 95 | loss: 3.350301\n",
      "epoch: 0 | individual: 95 | loss: 3.334054\n",
      "epoch: 0 | individual: 95 | loss: 3.377561\n",
      "epoch: 0 | individual: 96 | loss: 3.424533\n",
      "epoch: 0 | individual: 96 | loss: 3.422614\n",
      "epoch: 0 | individual: 96 | loss: 3.421839\n",
      "epoch: 0 | individual: 96 | loss: 3.422453\n",
      "epoch: 0 | individual: 96 | loss: 3.312094\n",
      "epoch: 0 | individual: 96 | loss: 3.359018\n",
      "epoch: 0 | individual: 97 | loss: 3.331383\n",
      "epoch: 0 | individual: 97 | loss: 3.366451\n",
      "epoch: 0 | individual: 97 | loss: 3.468101\n",
      "epoch: 0 | individual: 97 | loss: 3.353514\n",
      "epoch: 0 | individual: 97 | loss: 3.413822\n",
      "epoch: 0 | individual: 97 | loss: 3.371464\n",
      "epoch: 0 | individual: 98 | loss: 3.306386\n",
      "epoch: 0 | individual: 98 | loss: 3.379113\n",
      "epoch: 0 | individual: 98 | loss: 3.402072\n",
      "epoch: 0 | individual: 98 | loss: 3.311340\n",
      "epoch: 0 | individual: 98 | loss: 3.369299\n",
      "epoch: 0 | individual: 98 | loss: 3.395828\n",
      "epoch: 0 | individual: 99 | loss: 3.358507\n",
      "epoch: 0 | individual: 99 | loss: 3.320303\n",
      "epoch: 0 | individual: 99 | loss: 3.502166\n",
      "epoch: 0 | individual: 99 | loss: 3.310513\n",
      "epoch: 0 | individual: 99 | loss: 3.315836\n",
      "epoch: 0 | individual: 99 | loss: 3.363765\n",
      "epoch: 0 | individual: 100 | loss: 3.421154\n",
      "epoch: 0 | individual: 100 | loss: 3.441122\n",
      "epoch: 0 | individual: 100 | loss: 3.327908\n",
      "epoch: 0 | individual: 100 | loss: 3.349408\n",
      "epoch: 0 | individual: 100 | loss: 3.309646\n",
      "epoch: 0 | individual: 100 | loss: 3.365630\n",
      "epoch: 0 | individual: 101 | loss: 3.358322\n",
      "epoch: 0 | individual: 101 | loss: 3.353726\n",
      "epoch: 0 | individual: 101 | loss: 3.353900\n",
      "epoch: 0 | individual: 101 | loss: 3.561170\n",
      "epoch: 0 | individual: 101 | loss: 3.669958\n",
      "epoch: 0 | individual: 101 | loss: 3.332856\n",
      "epoch: 0 | individual: 102 | loss: 3.299239\n",
      "epoch: 0 | individual: 102 | loss: 3.369044\n",
      "epoch: 0 | individual: 102 | loss: 3.299372\n",
      "epoch: 0 | individual: 102 | loss: 3.296006\n",
      "epoch: 0 | individual: 102 | loss: 3.296033\n",
      "epoch: 0 | individual: 102 | loss: 3.299058\n",
      "epoch: 0 | individual: 103 | loss: 3.332050\n",
      "epoch: 0 | individual: 103 | loss: 3.292381\n",
      "epoch: 0 | individual: 103 | loss: 3.313353\n",
      "epoch: 0 | individual: 103 | loss: 3.319526\n",
      "epoch: 0 | individual: 103 | loss: 3.340754\n",
      "epoch: 0 | individual: 103 | loss: 3.326926\n",
      "epoch: 0 | individual: 104 | loss: 3.296849\n",
      "epoch: 0 | individual: 104 | loss: 3.320059\n",
      "epoch: 0 | individual: 104 | loss: 3.349945\n",
      "epoch: 0 | individual: 104 | loss: 3.394296\n",
      "epoch: 0 | individual: 104 | loss: 3.695060\n",
      "epoch: 0 | individual: 104 | loss: 4.194605\n",
      "epoch: 0 | individual: 105 | loss: 3.620500\n",
      "epoch: 0 | individual: 105 | loss: 3.307727\n",
      "epoch: 0 | individual: 105 | loss: 3.288066\n",
      "epoch: 0 | individual: 105 | loss: 3.351814\n",
      "epoch: 0 | individual: 105 | loss: 3.367841\n",
      "epoch: 0 | individual: 105 | loss: 3.290687\n",
      "epoch: 0 | individual: 106 | loss: 3.283303\n",
      "epoch: 0 | individual: 106 | loss: 3.300730\n",
      "epoch: 0 | individual: 106 | loss: 3.319488\n",
      "epoch: 0 | individual: 106 | loss: 3.304439\n",
      "epoch: 0 | individual: 106 | loss: 3.288557\n",
      "epoch: 0 | individual: 106 | loss: 3.332873\n",
      "epoch: 0 | individual: 107 | loss: 3.310354\n",
      "epoch: 0 | individual: 107 | loss: 3.342813\n",
      "epoch: 0 | individual: 107 | loss: 3.518797\n",
      "epoch: 0 | individual: 107 | loss: 3.405611\n",
      "epoch: 0 | individual: 107 | loss: 3.286278\n",
      "epoch: 0 | individual: 107 | loss: 3.340232\n",
      "epoch: 0 | individual: 108 | loss: 3.317795\n",
      "epoch: 0 | individual: 108 | loss: 3.306224\n",
      "epoch: 0 | individual: 108 | loss: 3.407308\n",
      "epoch: 0 | individual: 108 | loss: 3.337688\n",
      "epoch: 0 | individual: 108 | loss: 3.304320\n",
      "epoch: 0 | individual: 108 | loss: 3.373462\n",
      "epoch: 0 | individual: 109 | loss: 3.331627\n",
      "epoch: 0 | individual: 109 | loss: 3.306458\n",
      "epoch: 0 | individual: 109 | loss: 3.331669\n",
      "epoch: 0 | individual: 109 | loss: 3.824589\n",
      "epoch: 0 | individual: 109 | loss: 4.071440\n",
      "epoch: 0 | individual: 109 | loss: 3.640928\n",
      "epoch: 0 | individual: 110 | loss: 3.294143\n",
      "epoch: 0 | individual: 110 | loss: 3.277437\n",
      "epoch: 0 | individual: 110 | loss: 3.269258\n",
      "epoch: 0 | individual: 110 | loss: 3.329082\n",
      "epoch: 0 | individual: 110 | loss: 3.294496\n",
      "epoch: 0 | individual: 110 | loss: 3.304509\n",
      "epoch: 0 | individual: 111 | loss: 3.415774\n",
      "epoch: 0 | individual: 111 | loss: 3.291848\n",
      "epoch: 0 | individual: 111 | loss: 3.297300\n",
      "epoch: 0 | individual: 111 | loss: 3.301114\n",
      "epoch: 0 | individual: 111 | loss: 3.363278\n",
      "epoch: 0 | individual: 111 | loss: 3.330610\n",
      "epoch: 0 | individual: 112 | loss: 3.330283\n",
      "epoch: 0 | individual: 112 | loss: 3.328570\n",
      "epoch: 0 | individual: 112 | loss: 3.413196\n",
      "epoch: 0 | individual: 112 | loss: 3.450566\n",
      "epoch: 0 | individual: 112 | loss: 3.399124\n",
      "epoch: 0 | individual: 112 | loss: 3.287854\n",
      "epoch: 0 | individual: 113 | loss: 3.477287\n",
      "epoch: 0 | individual: 113 | loss: 3.327618\n",
      "epoch: 0 | individual: 113 | loss: 3.296769\n",
      "epoch: 0 | individual: 113 | loss: 3.279658\n",
      "epoch: 0 | individual: 113 | loss: 3.283168\n",
      "epoch: 0 | individual: 113 | loss: 3.296151\n",
      "epoch: 0 | individual: 114 | loss: 3.767764\n",
      "epoch: 0 | individual: 114 | loss: 3.399588\n",
      "epoch: 0 | individual: 114 | loss: 3.698017\n",
      "epoch: 0 | individual: 114 | loss: 3.542666\n",
      "epoch: 0 | individual: 114 | loss: 4.200146\n",
      "epoch: 0 | individual: 114 | loss: 3.270204\n",
      "epoch: 0 | individual: 115 | loss: 5.005181\n",
      "epoch: 0 | individual: 115 | loss: 5.917113\n",
      "epoch: 0 | individual: 115 | loss: 4.683889\n",
      "epoch: 0 | individual: 115 | loss: 4.038609\n",
      "epoch: 0 | individual: 115 | loss: 5.889649\n",
      "epoch: 0 | individual: 115 | loss: 3.377303\n",
      "epoch: 0 | individual: 116 | loss: 3.309857\n",
      "epoch: 0 | individual: 116 | loss: 3.301452\n",
      "epoch: 0 | individual: 116 | loss: 3.288819\n",
      "epoch: 0 | individual: 116 | loss: 3.276164\n",
      "epoch: 0 | individual: 116 | loss: 3.279624\n",
      "epoch: 0 | individual: 116 | loss: 3.321363\n",
      "epoch: 0 | individual: 117 | loss: 3.316772\n",
      "epoch: 0 | individual: 117 | loss: 3.326181\n",
      "epoch: 0 | individual: 117 | loss: 3.268034\n",
      "epoch: 0 | individual: 117 | loss: 3.549452\n",
      "epoch: 0 | individual: 117 | loss: 3.275363\n",
      "epoch: 0 | individual: 117 | loss: 3.292140\n",
      "epoch: 0 | individual: 118 | loss: 3.272984\n",
      "epoch: 0 | individual: 118 | loss: 3.389777\n",
      "epoch: 0 | individual: 118 | loss: 3.282144\n",
      "epoch: 0 | individual: 118 | loss: 3.361523\n",
      "epoch: 0 | individual: 118 | loss: 3.274712\n",
      "epoch: 0 | individual: 118 | loss: 3.270479\n",
      "epoch: 0 | individual: 119 | loss: 3.281175\n",
      "epoch: 0 | individual: 119 | loss: 3.473212\n",
      "epoch: 0 | individual: 119 | loss: 3.276206\n",
      "epoch: 0 | individual: 119 | loss: 3.365980\n",
      "epoch: 0 | individual: 119 | loss: 3.289936\n",
      "epoch: 0 | individual: 119 | loss: 3.312038\n",
      "epoch: 0 | individual: 120 | loss: 3.314616\n",
      "epoch: 0 | individual: 120 | loss: 3.371539\n",
      "epoch: 0 | individual: 120 | loss: 3.299322\n",
      "epoch: 0 | individual: 120 | loss: 3.350075\n",
      "epoch: 0 | individual: 120 | loss: 3.480644\n",
      "epoch: 0 | individual: 120 | loss: 3.542069\n",
      "epoch: 0 | individual: 121 | loss: 3.455436\n",
      "epoch: 0 | individual: 121 | loss: 3.415068\n",
      "epoch: 0 | individual: 121 | loss: 3.610177\n",
      "epoch: 0 | individual: 121 | loss: 3.327344\n",
      "epoch: 0 | individual: 121 | loss: 3.265090\n",
      "epoch: 0 | individual: 121 | loss: 3.261833\n",
      "epoch: 0 | individual: 122 | loss: 3.301448\n",
      "epoch: 0 | individual: 122 | loss: 3.296347\n",
      "epoch: 0 | individual: 122 | loss: 3.345535\n",
      "epoch: 0 | individual: 122 | loss: 3.359558\n",
      "epoch: 0 | individual: 122 | loss: 3.281677\n",
      "epoch: 0 | individual: 122 | loss: 3.350830\n",
      "epoch: 0 | individual: 123 | loss: 3.258345\n",
      "epoch: 0 | individual: 123 | loss: 3.285184\n",
      "epoch: 0 | individual: 123 | loss: 3.323431\n",
      "epoch: 0 | individual: 123 | loss: 3.259985\n",
      "epoch: 0 | individual: 123 | loss: 3.249179\n",
      "epoch: 0 | individual: 123 | loss: 3.256241\n",
      "epoch: 0 | individual: 124 | loss: 3.485192\n",
      "epoch: 0 | individual: 124 | loss: 3.331817\n",
      "epoch: 0 | individual: 124 | loss: 3.478606\n",
      "epoch: 0 | individual: 124 | loss: 3.266140\n",
      "epoch: 0 | individual: 124 | loss: 3.306530\n",
      "epoch: 0 | individual: 124 | loss: 3.298074\n",
      "epoch: 0 | individual: 125 | loss: 3.299043\n",
      "epoch: 0 | individual: 125 | loss: 3.270478\n",
      "epoch: 0 | individual: 125 | loss: 3.380893\n",
      "epoch: 0 | individual: 125 | loss: 3.366659\n",
      "epoch: 0 | individual: 125 | loss: 3.269182\n",
      "epoch: 0 | individual: 125 | loss: 3.303855\n",
      "epoch: 0 | individual: 126 | loss: 3.387624\n",
      "epoch: 0 | individual: 126 | loss: 3.292110\n",
      "epoch: 0 | individual: 126 | loss: 3.725622\n",
      "epoch: 0 | individual: 126 | loss: 3.279158\n",
      "epoch: 0 | individual: 126 | loss: 3.302351\n",
      "epoch: 0 | individual: 126 | loss: 3.297172\n",
      "epoch: 0 | individual: 127 | loss: 4.928110\n",
      "epoch: 0 | individual: 127 | loss: 3.486677\n",
      "epoch: 0 | individual: 127 | loss: 3.363039\n",
      "epoch: 0 | individual: 127 | loss: 3.273740\n",
      "epoch: 0 | individual: 127 | loss: 3.309073\n",
      "epoch: 0 | individual: 127 | loss: 3.281397\n",
      "epoch: 0 | individual: 128 | loss: 3.310029\n",
      "epoch: 0 | individual: 128 | loss: 3.379050\n",
      "epoch: 0 | individual: 128 | loss: 3.401007\n",
      "epoch: 0 | individual: 128 | loss: 3.312433\n",
      "epoch: 0 | individual: 128 | loss: 3.323759\n",
      "epoch: 0 | individual: 128 | loss: 3.416564\n",
      "epoch: 0 | individual: 129 | loss: 3.277763\n",
      "epoch: 0 | individual: 129 | loss: 3.410012\n",
      "epoch: 0 | individual: 129 | loss: 3.506696\n",
      "epoch: 0 | individual: 129 | loss: 3.713933\n",
      "epoch: 0 | individual: 129 | loss: 3.549709\n",
      "epoch: 0 | individual: 129 | loss: 3.547693\n",
      "epoch: 0 | individual: 130 | loss: 3.721187\n",
      "epoch: 0 | individual: 130 | loss: 3.830380\n",
      "epoch: 0 | individual: 130 | loss: 3.887188\n",
      "epoch: 0 | individual: 130 | loss: 3.765779\n",
      "epoch: 0 | individual: 130 | loss: 3.359043\n",
      "epoch: 0 | individual: 130 | loss: 3.651868\n",
      "epoch: 0 | individual: 131 | loss: 3.252489\n",
      "epoch: 0 | individual: 131 | loss: 3.580025\n",
      "epoch: 0 | individual: 131 | loss: 4.881503\n",
      "epoch: 0 | individual: 131 | loss: 3.269302\n",
      "epoch: 0 | individual: 131 | loss: 3.290570\n",
      "epoch: 0 | individual: 131 | loss: 3.294271\n",
      "epoch: 0 | individual: 132 | loss: 3.267621\n",
      "epoch: 0 | individual: 132 | loss: 3.266272\n",
      "epoch: 0 | individual: 132 | loss: 3.553855\n",
      "epoch: 0 | individual: 132 | loss: 4.487151\n",
      "epoch: 0 | individual: 132 | loss: 5.320725\n",
      "epoch: 0 | individual: 132 | loss: 3.300920\n",
      "epoch: 0 | individual: 133 | loss: 3.264074\n",
      "epoch: 0 | individual: 133 | loss: 3.268056\n",
      "epoch: 0 | individual: 133 | loss: 3.280660\n",
      "epoch: 0 | individual: 133 | loss: 3.284627\n",
      "epoch: 0 | individual: 133 | loss: 3.297113\n",
      "epoch: 0 | individual: 133 | loss: 3.295529\n",
      "epoch: 0 | individual: 134 | loss: 3.266091\n",
      "epoch: 0 | individual: 134 | loss: 3.262600\n",
      "epoch: 0 | individual: 134 | loss: 3.260522\n",
      "epoch: 0 | individual: 134 | loss: 3.255266\n",
      "epoch: 0 | individual: 134 | loss: 3.337182\n",
      "epoch: 0 | individual: 134 | loss: 3.277917\n",
      "epoch: 0 | individual: 135 | loss: 3.350681\n",
      "epoch: 0 | individual: 135 | loss: 3.516154\n",
      "epoch: 0 | individual: 135 | loss: 3.344688\n",
      "epoch: 0 | individual: 135 | loss: 3.385587\n",
      "epoch: 0 | individual: 135 | loss: 3.514337\n",
      "epoch: 0 | individual: 135 | loss: 3.297133\n",
      "epoch: 0 | individual: 136 | loss: 3.257623\n",
      "epoch: 0 | individual: 136 | loss: 3.244729\n",
      "epoch: 0 | individual: 136 | loss: 3.243711\n",
      "epoch: 0 | individual: 136 | loss: 3.245880\n",
      "epoch: 0 | individual: 136 | loss: 3.294190\n",
      "epoch: 0 | individual: 136 | loss: 3.240023\n",
      "epoch: 0 | individual: 137 | loss: 3.290461\n",
      "epoch: 0 | individual: 137 | loss: 3.288010\n",
      "epoch: 0 | individual: 137 | loss: 3.282492\n",
      "epoch: 0 | individual: 137 | loss: 3.277373\n",
      "epoch: 0 | individual: 137 | loss: 3.239494\n",
      "epoch: 0 | individual: 137 | loss: 3.291531\n",
      "epoch: 0 | individual: 138 | loss: 3.276481\n",
      "epoch: 0 | individual: 138 | loss: 3.448270\n",
      "epoch: 0 | individual: 138 | loss: 3.437748\n",
      "epoch: 0 | individual: 138 | loss: 3.258366\n",
      "epoch: 0 | individual: 138 | loss: 3.277873\n",
      "epoch: 0 | individual: 138 | loss: 3.282951\n",
      "epoch: 0 | individual: 139 | loss: 3.277934\n",
      "epoch: 0 | individual: 139 | loss: 3.306230\n",
      "epoch: 0 | individual: 139 | loss: 3.338320\n",
      "epoch: 0 | individual: 139 | loss: 3.414860\n",
      "epoch: 0 | individual: 139 | loss: 3.257893\n",
      "epoch: 0 | individual: 139 | loss: 3.380211\n",
      "epoch: 0 | individual: 140 | loss: 3.280924\n",
      "epoch: 0 | individual: 140 | loss: 3.400851\n",
      "epoch: 0 | individual: 140 | loss: 3.226318\n",
      "epoch: 0 | individual: 140 | loss: 3.236092\n",
      "epoch: 0 | individual: 140 | loss: 3.252207\n",
      "epoch: 0 | individual: 140 | loss: 3.249310\n",
      "epoch: 0 | individual: 141 | loss: 3.567290\n",
      "epoch: 0 | individual: 141 | loss: 3.287094\n",
      "epoch: 0 | individual: 141 | loss: 3.323899\n",
      "epoch: 0 | individual: 141 | loss: 3.413481\n",
      "epoch: 0 | individual: 141 | loss: 3.286675\n",
      "epoch: 0 | individual: 141 | loss: 3.277595\n",
      "epoch: 0 | individual: 142 | loss: 3.320884\n",
      "epoch: 0 | individual: 142 | loss: 3.476615\n",
      "epoch: 0 | individual: 142 | loss: 3.256487\n",
      "epoch: 0 | individual: 142 | loss: 3.255281\n",
      "epoch: 0 | individual: 142 | loss: 3.285502\n",
      "epoch: 0 | individual: 142 | loss: 3.274883\n",
      "epoch: 0 | individual: 143 | loss: 3.277592\n",
      "epoch: 0 | individual: 143 | loss: 3.260931\n",
      "epoch: 0 | individual: 143 | loss: 3.241633\n",
      "epoch: 0 | individual: 143 | loss: 3.272238\n",
      "epoch: 0 | individual: 143 | loss: 3.358125\n",
      "epoch: 0 | individual: 143 | loss: 3.233309\n",
      "epoch: 0 | individual: 144 | loss: 3.456127\n",
      "epoch: 0 | individual: 144 | loss: 4.298771\n",
      "epoch: 0 | individual: 144 | loss: 3.231067\n",
      "epoch: 0 | individual: 144 | loss: 3.231749\n",
      "epoch: 0 | individual: 144 | loss: 3.363782\n",
      "epoch: 0 | individual: 144 | loss: 3.281661\n",
      "epoch: 0 | individual: 145 | loss: 3.236586\n",
      "epoch: 0 | individual: 145 | loss: 3.488389\n",
      "epoch: 0 | individual: 145 | loss: 4.305302\n",
      "epoch: 0 | individual: 145 | loss: 4.183979\n",
      "epoch: 0 | individual: 145 | loss: 3.508898\n",
      "epoch: 0 | individual: 145 | loss: 3.414349\n",
      "epoch: 0 | individual: 146 | loss: 3.253997\n",
      "epoch: 0 | individual: 146 | loss: 3.253388\n",
      "epoch: 0 | individual: 146 | loss: 3.333421\n",
      "epoch: 0 | individual: 146 | loss: 3.242285\n",
      "epoch: 0 | individual: 146 | loss: 3.276083\n",
      "epoch: 0 | individual: 146 | loss: 3.245311\n",
      "epoch: 0 | individual: 147 | loss: 3.285988\n",
      "epoch: 0 | individual: 147 | loss: 3.284576\n",
      "epoch: 0 | individual: 147 | loss: 3.375602\n",
      "epoch: 0 | individual: 147 | loss: 3.284153\n",
      "epoch: 0 | individual: 147 | loss: 3.395849\n",
      "epoch: 0 | individual: 147 | loss: 3.238174\n",
      "epoch: 0 | individual: 148 | loss: 3.256639\n",
      "epoch: 0 | individual: 148 | loss: 3.256328\n",
      "epoch: 0 | individual: 148 | loss: 3.255736\n",
      "epoch: 0 | individual: 148 | loss: 3.255927\n",
      "epoch: 0 | individual: 148 | loss: 3.284802\n",
      "epoch: 0 | individual: 148 | loss: 3.279344\n",
      "epoch: 0 | individual: 149 | loss: 3.356872\n",
      "epoch: 0 | individual: 149 | loss: 3.272661\n",
      "epoch: 0 | individual: 149 | loss: 3.272326\n",
      "epoch: 0 | individual: 149 | loss: 3.271922\n",
      "epoch: 0 | individual: 149 | loss: 3.251370\n",
      "epoch: 0 | individual: 149 | loss: 3.279257\n",
      "epoch: 0 | individual: 150 | loss: 3.273750\n",
      "epoch: 0 | individual: 150 | loss: 3.441297\n",
      "epoch: 0 | individual: 150 | loss: 3.278974\n",
      "epoch: 0 | individual: 150 | loss: 3.355127\n",
      "epoch: 0 | individual: 150 | loss: 3.278555\n",
      "epoch: 0 | individual: 150 | loss: 3.270933\n",
      "epoch: 0 | individual: 151 | loss: 3.425376\n",
      "epoch: 0 | individual: 151 | loss: 3.705201\n",
      "epoch: 0 | individual: 151 | loss: 3.695088\n",
      "epoch: 0 | individual: 151 | loss: 3.227455\n",
      "epoch: 0 | individual: 151 | loss: 3.261496\n",
      "epoch: 0 | individual: 151 | loss: 3.231965\n",
      "epoch: 0 | individual: 152 | loss: 3.276971\n",
      "epoch: 0 | individual: 152 | loss: 3.270178\n",
      "epoch: 0 | individual: 152 | loss: 3.263377\n",
      "epoch: 0 | individual: 152 | loss: 3.271359\n",
      "epoch: 0 | individual: 152 | loss: 3.326940\n",
      "epoch: 0 | individual: 152 | loss: 3.540651\n",
      "epoch: 0 | individual: 153 | loss: 3.562302\n",
      "epoch: 0 | individual: 153 | loss: 3.250976\n",
      "epoch: 0 | individual: 153 | loss: 3.249767\n",
      "epoch: 0 | individual: 153 | loss: 3.457988\n",
      "epoch: 0 | individual: 153 | loss: 3.484762\n",
      "epoch: 0 | individual: 153 | loss: 3.275034\n",
      "epoch: 0 | individual: 154 | loss: 3.250315\n",
      "epoch: 0 | individual: 154 | loss: 3.260408\n",
      "epoch: 0 | individual: 154 | loss: 3.269280\n",
      "epoch: 0 | individual: 154 | loss: 3.513128\n",
      "epoch: 0 | individual: 154 | loss: 3.259399\n",
      "epoch: 0 | individual: 154 | loss: 3.310177\n",
      "epoch: 0 | individual: 155 | loss: 3.272403\n",
      "epoch: 0 | individual: 155 | loss: 3.240794\n",
      "epoch: 0 | individual: 155 | loss: 3.265645\n",
      "epoch: 0 | individual: 155 | loss: 3.253133\n",
      "epoch: 0 | individual: 155 | loss: 3.267240\n",
      "epoch: 0 | individual: 155 | loss: 3.247076\n",
      "epoch: 0 | individual: 156 | loss: 3.225900\n",
      "epoch: 0 | individual: 156 | loss: 3.260769\n",
      "epoch: 0 | individual: 156 | loss: 3.227088\n",
      "epoch: 0 | individual: 156 | loss: 3.271378\n",
      "epoch: 0 | individual: 156 | loss: 3.257434\n",
      "epoch: 0 | individual: 156 | loss: 3.223384\n",
      "epoch: 0 | individual: 157 | loss: 3.354677\n",
      "epoch: 0 | individual: 157 | loss: 3.354641\n",
      "epoch: 0 | individual: 157 | loss: 3.354565\n",
      "epoch: 0 | individual: 157 | loss: 3.354455\n",
      "epoch: 0 | individual: 157 | loss: 3.224465\n",
      "epoch: 0 | individual: 157 | loss: 3.266973\n",
      "epoch: 0 | individual: 158 | loss: 3.362700\n",
      "epoch: 0 | individual: 158 | loss: 3.557573\n",
      "epoch: 0 | individual: 158 | loss: 4.198423\n",
      "epoch: 0 | individual: 158 | loss: 3.381626\n",
      "epoch: 0 | individual: 158 | loss: 4.326863\n",
      "epoch: 0 | individual: 158 | loss: 4.353414\n",
      "epoch: 0 | individual: 159 | loss: 3.261796\n",
      "epoch: 0 | individual: 159 | loss: 3.261138\n",
      "epoch: 0 | individual: 159 | loss: 3.260407\n",
      "epoch: 0 | individual: 159 | loss: 3.264206\n",
      "epoch: 0 | individual: 159 | loss: 3.270835\n",
      "epoch: 0 | individual: 159 | loss: 3.270104\n",
      "epoch: 0 | individual: 160 | loss: 3.267488\n",
      "epoch: 0 | individual: 160 | loss: 3.228807\n",
      "epoch: 0 | individual: 160 | loss: 3.239841\n",
      "epoch: 0 | individual: 160 | loss: 3.269786\n",
      "epoch: 0 | individual: 160 | loss: 3.264074\n",
      "epoch: 0 | individual: 160 | loss: 3.223010\n",
      "epoch: 0 | individual: 161 | loss: 3.268822\n",
      "epoch: 0 | individual: 161 | loss: 3.265085\n",
      "epoch: 0 | individual: 161 | loss: 3.261845\n",
      "epoch: 0 | individual: 161 | loss: 3.259001\n",
      "epoch: 0 | individual: 161 | loss: 3.260481\n",
      "epoch: 0 | individual: 161 | loss: 3.412184\n",
      "epoch: 0 | individual: 162 | loss: 3.221908\n",
      "epoch: 0 | individual: 162 | loss: 3.221525\n",
      "epoch: 0 | individual: 162 | loss: 3.249300\n",
      "epoch: 0 | individual: 162 | loss: 3.264376\n",
      "epoch: 0 | individual: 162 | loss: 3.259034\n",
      "epoch: 0 | individual: 162 | loss: 3.262715\n",
      "epoch: 0 | individual: 163 | loss: 3.260665\n",
      "epoch: 0 | individual: 163 | loss: 3.386610\n",
      "epoch: 0 | individual: 163 | loss: 3.234288\n",
      "epoch: 0 | individual: 163 | loss: 3.514269\n",
      "epoch: 0 | individual: 163 | loss: 3.256773\n",
      "epoch: 0 | individual: 163 | loss: 3.328824\n",
      "epoch: 0 | individual: 164 | loss: 3.231154\n",
      "epoch: 0 | individual: 164 | loss: 3.230843\n",
      "epoch: 0 | individual: 164 | loss: 3.259349\n",
      "epoch: 0 | individual: 164 | loss: 3.261610\n",
      "epoch: 0 | individual: 164 | loss: 3.258585\n",
      "epoch: 0 | individual: 164 | loss: 3.263578\n",
      "epoch: 0 | individual: 165 | loss: 3.227736\n",
      "epoch: 0 | individual: 165 | loss: 3.236522\n",
      "epoch: 0 | individual: 165 | loss: 3.235420\n",
      "epoch: 0 | individual: 165 | loss: 3.256682\n",
      "epoch: 0 | individual: 165 | loss: 3.216798\n",
      "epoch: 0 | individual: 165 | loss: 3.396509\n",
      "epoch: 0 | individual: 166 | loss: 3.206385\n",
      "epoch: 0 | individual: 166 | loss: 3.525825\n",
      "epoch: 0 | individual: 166 | loss: 3.366028\n",
      "epoch: 0 | individual: 166 | loss: 3.217185\n",
      "epoch: 0 | individual: 166 | loss: 3.259793\n",
      "epoch: 0 | individual: 166 | loss: 3.267579\n",
      "epoch: 0 | individual: 167 | loss: 3.218333\n",
      "epoch: 0 | individual: 167 | loss: 3.227470\n",
      "epoch: 0 | individual: 167 | loss: 3.259129\n",
      "epoch: 0 | individual: 167 | loss: 3.330852\n",
      "epoch: 0 | individual: 167 | loss: 3.215325\n",
      "epoch: 0 | individual: 167 | loss: 3.258662\n",
      "epoch: 0 | individual: 168 | loss: 3.292094\n",
      "epoch: 0 | individual: 168 | loss: 3.222936\n",
      "epoch: 0 | individual: 168 | loss: 3.237046\n",
      "epoch: 0 | individual: 168 | loss: 3.273422\n",
      "epoch: 0 | individual: 168 | loss: 3.214200\n",
      "epoch: 0 | individual: 168 | loss: 3.257863\n",
      "epoch: 0 | individual: 169 | loss: 3.649906\n",
      "epoch: 0 | individual: 169 | loss: 3.557942\n",
      "epoch: 0 | individual: 169 | loss: 3.399733\n",
      "epoch: 0 | individual: 169 | loss: 3.469486\n",
      "epoch: 0 | individual: 169 | loss: 3.255059\n",
      "epoch: 0 | individual: 169 | loss: 3.257056\n",
      "epoch: 0 | individual: 170 | loss: 3.257142\n",
      "epoch: 0 | individual: 170 | loss: 3.254416\n",
      "epoch: 0 | individual: 170 | loss: 3.252022\n",
      "epoch: 0 | individual: 170 | loss: 3.251563\n",
      "epoch: 0 | individual: 170 | loss: 3.585747\n",
      "epoch: 0 | individual: 170 | loss: 3.991212\n",
      "epoch: 0 | individual: 171 | loss: 3.332819\n",
      "epoch: 0 | individual: 171 | loss: 3.207943\n",
      "epoch: 0 | individual: 171 | loss: 3.214722\n",
      "epoch: 0 | individual: 171 | loss: 3.245015\n",
      "epoch: 0 | individual: 171 | loss: 3.254801\n",
      "epoch: 0 | individual: 171 | loss: 3.219074\n",
      "epoch: 0 | individual: 172 | loss: 3.296273\n",
      "epoch: 0 | individual: 172 | loss: 3.387993\n",
      "epoch: 0 | individual: 172 | loss: 3.301287\n",
      "epoch: 0 | individual: 172 | loss: 3.254465\n",
      "epoch: 0 | individual: 172 | loss: 3.252623\n",
      "epoch: 0 | individual: 172 | loss: 3.276834\n",
      "epoch: 0 | individual: 173 | loss: 3.243740\n",
      "epoch: 0 | individual: 173 | loss: 3.243922\n",
      "epoch: 0 | individual: 173 | loss: 3.712432\n",
      "epoch: 0 | individual: 173 | loss: 3.237159\n",
      "epoch: 0 | individual: 173 | loss: 3.302799\n",
      "epoch: 0 | individual: 173 | loss: 3.329841\n",
      "epoch: 0 | individual: 174 | loss: 3.256749\n",
      "epoch: 0 | individual: 174 | loss: 3.250331\n",
      "epoch: 0 | individual: 174 | loss: 3.232048\n",
      "epoch: 0 | individual: 174 | loss: 3.220436\n",
      "epoch: 0 | individual: 174 | loss: 3.220298\n",
      "epoch: 0 | individual: 174 | loss: 3.217918\n",
      "epoch: 0 | individual: 175 | loss: 3.253993\n",
      "epoch: 0 | individual: 175 | loss: 3.251527\n",
      "epoch: 0 | individual: 175 | loss: 3.249376\n",
      "epoch: 0 | individual: 175 | loss: 3.248131\n",
      "epoch: 0 | individual: 175 | loss: 3.233959\n",
      "epoch: 0 | individual: 175 | loss: 3.254969\n",
      "epoch: 0 | individual: 176 | loss: 3.238344\n",
      "epoch: 0 | individual: 176 | loss: 3.238600\n",
      "epoch: 0 | individual: 176 | loss: 3.238319\n",
      "epoch: 0 | individual: 176 | loss: 3.237691\n",
      "epoch: 0 | individual: 176 | loss: 3.215127\n",
      "epoch: 0 | individual: 176 | loss: 3.254249\n",
      "epoch: 0 | individual: 177 | loss: 3.236201\n",
      "epoch: 0 | individual: 177 | loss: 3.239784\n",
      "epoch: 0 | individual: 177 | loss: 3.236020\n",
      "epoch: 0 | individual: 177 | loss: 3.235339\n",
      "epoch: 0 | individual: 177 | loss: 3.224470\n",
      "epoch: 0 | individual: 177 | loss: 3.253286\n",
      "epoch: 0 | individual: 178 | loss: 3.233372\n",
      "epoch: 0 | individual: 178 | loss: 3.235609\n",
      "epoch: 0 | individual: 178 | loss: 3.248689\n",
      "epoch: 0 | individual: 178 | loss: 3.299167\n",
      "epoch: 0 | individual: 178 | loss: 3.430835\n",
      "epoch: 0 | individual: 178 | loss: 3.253147\n",
      "epoch: 0 | individual: 179 | loss: 3.247782\n",
      "epoch: 0 | individual: 179 | loss: 3.246386\n",
      "epoch: 0 | individual: 179 | loss: 3.208327\n",
      "epoch: 0 | individual: 179 | loss: 3.251751\n",
      "epoch: 0 | individual: 179 | loss: 3.242118\n",
      "epoch: 0 | individual: 179 | loss: 3.213068\n",
      "epoch: 0 | individual: 180 | loss: 3.277041\n",
      "epoch: 0 | individual: 180 | loss: 3.273902\n",
      "epoch: 0 | individual: 180 | loss: 3.215857\n",
      "epoch: 0 | individual: 180 | loss: 3.386232\n",
      "epoch: 0 | individual: 180 | loss: 3.216591\n",
      "epoch: 0 | individual: 180 | loss: 3.241023\n",
      "epoch: 0 | individual: 181 | loss: 3.279359\n",
      "epoch: 0 | individual: 181 | loss: 3.279423\n",
      "epoch: 0 | individual: 181 | loss: 3.279745\n",
      "epoch: 0 | individual: 181 | loss: 3.253394\n",
      "epoch: 0 | individual: 181 | loss: 3.243988\n",
      "epoch: 0 | individual: 181 | loss: 3.247598\n",
      "epoch: 0 | individual: 182 | loss: 3.247931\n",
      "epoch: 0 | individual: 182 | loss: 3.292764\n",
      "epoch: 0 | individual: 182 | loss: 3.227499\n",
      "epoch: 0 | individual: 182 | loss: 3.287968\n",
      "epoch: 0 | individual: 182 | loss: 3.253147\n",
      "epoch: 0 | individual: 182 | loss: 3.247503\n",
      "epoch: 0 | individual: 183 | loss: 3.240015\n",
      "epoch: 0 | individual: 183 | loss: 3.239145\n",
      "epoch: 0 | individual: 183 | loss: 3.239073\n",
      "epoch: 0 | individual: 183 | loss: 3.242739\n",
      "epoch: 0 | individual: 183 | loss: 3.240925\n",
      "epoch: 0 | individual: 183 | loss: 3.401829\n",
      "epoch: 0 | individual: 184 | loss: 3.232856\n",
      "epoch: 0 | individual: 184 | loss: 3.230460\n",
      "epoch: 0 | individual: 184 | loss: 3.394552\n",
      "epoch: 0 | individual: 184 | loss: 3.232162\n",
      "epoch: 0 | individual: 184 | loss: 3.210714\n",
      "epoch: 0 | individual: 184 | loss: 3.211480\n",
      "epoch: 0 | individual: 185 | loss: 3.252080\n",
      "epoch: 0 | individual: 185 | loss: 3.243952\n",
      "epoch: 0 | individual: 185 | loss: 3.236302\n",
      "epoch: 0 | individual: 185 | loss: 3.233125\n",
      "epoch: 0 | individual: 185 | loss: 3.296704\n",
      "epoch: 0 | individual: 185 | loss: 3.238786\n",
      "epoch: 0 | individual: 186 | loss: 3.217373\n",
      "epoch: 0 | individual: 186 | loss: 3.214823\n",
      "epoch: 0 | individual: 186 | loss: 3.214317\n",
      "epoch: 0 | individual: 186 | loss: 3.206895\n",
      "epoch: 0 | individual: 186 | loss: 3.205347\n",
      "epoch: 0 | individual: 186 | loss: 3.211356\n",
      "epoch: 0 | individual: 187 | loss: 3.282185\n",
      "epoch: 0 | individual: 187 | loss: 3.224233\n",
      "epoch: 0 | individual: 187 | loss: 3.249666\n",
      "epoch: 0 | individual: 187 | loss: 3.539186\n",
      "epoch: 0 | individual: 187 | loss: 3.245997\n",
      "epoch: 0 | individual: 187 | loss: 3.251276\n",
      "epoch: 0 | individual: 188 | loss: 3.246820\n",
      "epoch: 0 | individual: 188 | loss: 3.350987\n",
      "epoch: 0 | individual: 188 | loss: 3.346519\n",
      "epoch: 0 | individual: 188 | loss: 3.230963\n",
      "epoch: 0 | individual: 188 | loss: 3.229160\n",
      "epoch: 0 | individual: 188 | loss: 3.243664\n",
      "epoch: 0 | individual: 189 | loss: 3.263292\n",
      "epoch: 0 | individual: 189 | loss: 3.308724\n",
      "epoch: 0 | individual: 189 | loss: 3.247060\n",
      "epoch: 0 | individual: 189 | loss: 3.213326\n",
      "epoch: 0 | individual: 189 | loss: 3.235809\n",
      "epoch: 0 | individual: 189 | loss: 3.251210\n",
      "epoch: 0 | individual: 190 | loss: 3.246367\n",
      "epoch: 0 | individual: 190 | loss: 3.244254\n",
      "epoch: 0 | individual: 190 | loss: 3.239805\n",
      "epoch: 0 | individual: 190 | loss: 3.234658\n",
      "epoch: 0 | individual: 190 | loss: 3.236311\n",
      "epoch: 0 | individual: 190 | loss: 3.238286\n",
      "epoch: 0 | individual: 191 | loss: 3.240767\n",
      "epoch: 0 | individual: 191 | loss: 3.246143\n",
      "epoch: 0 | individual: 191 | loss: 3.238460\n",
      "epoch: 0 | individual: 191 | loss: 3.232266\n",
      "epoch: 0 | individual: 191 | loss: 3.264723\n",
      "epoch: 0 | individual: 191 | loss: 3.288759\n",
      "epoch: 0 | individual: 192 | loss: 3.248116\n",
      "epoch: 0 | individual: 192 | loss: 3.242882\n",
      "epoch: 0 | individual: 192 | loss: 3.263532\n",
      "epoch: 0 | individual: 192 | loss: 3.346582\n",
      "epoch: 0 | individual: 192 | loss: 3.204665\n",
      "epoch: 0 | individual: 192 | loss: 3.246348\n",
      "epoch: 0 | individual: 193 | loss: 3.241102\n",
      "epoch: 0 | individual: 193 | loss: 3.239397\n",
      "epoch: 0 | individual: 193 | loss: 3.240609\n",
      "epoch: 0 | individual: 193 | loss: 3.240366\n",
      "epoch: 0 | individual: 193 | loss: 3.209473\n",
      "epoch: 0 | individual: 193 | loss: 3.434239\n",
      "epoch: 0 | individual: 194 | loss: 3.198906\n",
      "epoch: 0 | individual: 194 | loss: 3.197914\n",
      "epoch: 0 | individual: 194 | loss: 3.197990\n",
      "epoch: 0 | individual: 194 | loss: 3.336588\n",
      "epoch: 0 | individual: 194 | loss: 3.444084\n",
      "epoch: 0 | individual: 194 | loss: 3.201551\n",
      "epoch: 0 | individual: 195 | loss: 3.208266\n",
      "epoch: 0 | individual: 195 | loss: 3.220264\n",
      "epoch: 0 | individual: 195 | loss: 3.400981\n",
      "epoch: 0 | individual: 195 | loss: 3.226521\n",
      "epoch: 0 | individual: 195 | loss: 3.237073\n",
      "epoch: 0 | individual: 195 | loss: 3.412849\n",
      "epoch: 0 | individual: 196 | loss: 3.322201\n",
      "epoch: 0 | individual: 196 | loss: 3.213804\n",
      "epoch: 0 | individual: 196 | loss: 3.337826\n",
      "epoch: 0 | individual: 196 | loss: 3.405374\n",
      "epoch: 0 | individual: 196 | loss: 3.288202\n",
      "epoch: 0 | individual: 196 | loss: 3.248262\n",
      "epoch: 0 | individual: 197 | loss: 3.404697\n",
      "epoch: 0 | individual: 197 | loss: 3.229241\n",
      "epoch: 0 | individual: 197 | loss: 3.247483\n",
      "epoch: 0 | individual: 197 | loss: 3.643878\n",
      "epoch: 0 | individual: 197 | loss: 3.231335\n",
      "epoch: 0 | individual: 197 | loss: 3.245131\n",
      "epoch: 0 | individual: 198 | loss: 3.231277\n",
      "epoch: 0 | individual: 198 | loss: 3.226719\n",
      "epoch: 0 | individual: 198 | loss: 3.230421\n",
      "epoch: 0 | individual: 198 | loss: 3.237872\n",
      "epoch: 0 | individual: 198 | loss: 3.236689\n",
      "epoch: 0 | individual: 198 | loss: 3.241746\n",
      "epoch: 0 | individual: 199 | loss: 3.217206\n",
      "epoch: 0 | individual: 199 | loss: 3.217358\n",
      "epoch: 0 | individual: 199 | loss: 3.216043\n",
      "epoch: 0 | individual: 199 | loss: 3.215894\n",
      "epoch: 0 | individual: 199 | loss: 3.238853\n",
      "epoch: 0 | individual: 199 | loss: 3.242233\n",
      "epoch: 0 | individual: 200 | loss: 3.245001\n",
      "epoch: 0 | individual: 200 | loss: 3.243074\n",
      "epoch: 0 | individual: 200 | loss: 3.238430\n",
      "epoch: 0 | individual: 200 | loss: 3.372326\n",
      "epoch: 0 | individual: 200 | loss: 3.249270\n",
      "epoch: 0 | individual: 200 | loss: 3.243156\n",
      "epoch: 0 | individual: 201 | loss: 3.279103\n",
      "epoch: 0 | individual: 201 | loss: 3.279092\n",
      "epoch: 0 | individual: 201 | loss: 3.278978\n",
      "epoch: 0 | individual: 201 | loss: 3.333966\n",
      "epoch: 0 | individual: 201 | loss: 3.209899\n",
      "epoch: 0 | individual: 201 | loss: 3.247908\n",
      "epoch: 0 | individual: 202 | loss: 3.222384\n",
      "epoch: 0 | individual: 202 | loss: 3.225712\n",
      "epoch: 0 | individual: 202 | loss: 3.229371\n",
      "epoch: 0 | individual: 202 | loss: 3.209611\n",
      "epoch: 0 | individual: 202 | loss: 3.367192\n",
      "epoch: 0 | individual: 202 | loss: 3.222430\n",
      "epoch: 0 | individual: 203 | loss: 3.211714\n",
      "epoch: 0 | individual: 203 | loss: 3.241542\n",
      "epoch: 0 | individual: 203 | loss: 3.205892\n",
      "epoch: 0 | individual: 203 | loss: 3.209388\n",
      "epoch: 0 | individual: 203 | loss: 3.246494\n",
      "epoch: 0 | individual: 203 | loss: 3.241345\n",
      "epoch: 0 | individual: 204 | loss: 3.300710\n",
      "epoch: 0 | individual: 204 | loss: 3.215337\n",
      "epoch: 0 | individual: 204 | loss: 3.389415\n",
      "epoch: 0 | individual: 204 | loss: 3.207527\n",
      "epoch: 0 | individual: 204 | loss: 3.208283\n",
      "epoch: 0 | individual: 204 | loss: 3.247187\n",
      "epoch: 0 | individual: 205 | loss: 3.223182\n",
      "epoch: 0 | individual: 205 | loss: 3.245181\n",
      "epoch: 0 | individual: 205 | loss: 3.888769\n",
      "epoch: 0 | individual: 205 | loss: 3.279341\n",
      "epoch: 0 | individual: 205 | loss: 3.304513\n",
      "epoch: 0 | individual: 205 | loss: 3.247199\n",
      "epoch: 0 | individual: 206 | loss: 3.369982\n",
      "epoch: 0 | individual: 206 | loss: 3.348722\n",
      "epoch: 0 | individual: 206 | loss: 3.302557\n",
      "epoch: 0 | individual: 206 | loss: 3.247489\n",
      "epoch: 0 | individual: 206 | loss: 3.257152\n",
      "epoch: 0 | individual: 206 | loss: 3.309102\n",
      "epoch: 0 | individual: 207 | loss: 3.215222\n",
      "epoch: 0 | individual: 207 | loss: 3.230065\n",
      "epoch: 0 | individual: 207 | loss: 3.229835\n",
      "epoch: 0 | individual: 207 | loss: 3.233036\n",
      "epoch: 0 | individual: 207 | loss: 3.263191\n",
      "epoch: 0 | individual: 207 | loss: 3.212826\n",
      "epoch: 0 | individual: 208 | loss: 3.357972\n",
      "epoch: 0 | individual: 208 | loss: 3.248320\n",
      "epoch: 0 | individual: 208 | loss: 3.476465\n",
      "epoch: 0 | individual: 208 | loss: 3.207250\n",
      "epoch: 0 | individual: 208 | loss: 3.248160\n",
      "epoch: 0 | individual: 208 | loss: 3.321151\n",
      "epoch: 0 | individual: 209 | loss: 3.219853\n",
      "epoch: 0 | individual: 209 | loss: 3.220032\n",
      "epoch: 0 | individual: 209 | loss: 3.219980\n",
      "epoch: 0 | individual: 209 | loss: 3.219592\n",
      "epoch: 0 | individual: 209 | loss: 3.244477\n",
      "epoch: 0 | individual: 209 | loss: 3.243162\n",
      "epoch: 0 | individual: 210 | loss: 3.248606\n",
      "epoch: 0 | individual: 210 | loss: 3.223306\n",
      "epoch: 0 | individual: 210 | loss: 3.458471\n",
      "epoch: 0 | individual: 210 | loss: 3.236913\n",
      "epoch: 0 | individual: 210 | loss: 3.340980\n",
      "epoch: 0 | individual: 210 | loss: 3.306967\n",
      "epoch: 0 | individual: 211 | loss: 3.216204\n",
      "epoch: 0 | individual: 211 | loss: 3.216170\n",
      "epoch: 0 | individual: 211 | loss: 3.215235\n",
      "epoch: 0 | individual: 211 | loss: 3.215099\n",
      "epoch: 0 | individual: 211 | loss: 3.238825\n",
      "epoch: 0 | individual: 211 | loss: 3.247080\n",
      "epoch: 0 | individual: 212 | loss: 3.344636\n",
      "epoch: 0 | individual: 212 | loss: 3.344449\n",
      "epoch: 0 | individual: 212 | loss: 3.344110\n",
      "epoch: 0 | individual: 212 | loss: 3.321738\n",
      "epoch: 0 | individual: 212 | loss: 3.266035\n",
      "epoch: 0 | individual: 212 | loss: 3.200999\n",
      "epoch: 0 | individual: 213 | loss: 3.202603\n",
      "epoch: 0 | individual: 213 | loss: 3.202724\n",
      "epoch: 0 | individual: 213 | loss: 3.202695\n",
      "epoch: 0 | individual: 213 | loss: 3.202537\n",
      "epoch: 0 | individual: 213 | loss: 3.212231\n",
      "epoch: 0 | individual: 213 | loss: 3.201214\n",
      "epoch: 0 | individual: 214 | loss: 3.747328\n",
      "epoch: 0 | individual: 214 | loss: 3.640061\n",
      "epoch: 0 | individual: 214 | loss: 3.350450\n",
      "epoch: 0 | individual: 214 | loss: 3.200739\n",
      "epoch: 0 | individual: 214 | loss: 3.397865\n",
      "epoch: 0 | individual: 214 | loss: 3.216947\n",
      "epoch: 0 | individual: 215 | loss: 3.451264\n",
      "epoch: 0 | individual: 215 | loss: 3.261374\n",
      "epoch: 0 | individual: 215 | loss: 3.266770\n",
      "epoch: 0 | individual: 215 | loss: 3.329253\n",
      "epoch: 0 | individual: 215 | loss: 3.252481\n",
      "epoch: 0 | individual: 215 | loss: 3.252956\n",
      "epoch: 0 | individual: 216 | loss: 3.218115\n",
      "epoch: 0 | individual: 216 | loss: 3.214256\n",
      "epoch: 0 | individual: 216 | loss: 3.499655\n",
      "epoch: 0 | individual: 216 | loss: 4.469044\n",
      "epoch: 0 | individual: 216 | loss: 5.255355\n",
      "epoch: 0 | individual: 216 | loss: 4.099668\n",
      "epoch: 0 | individual: 217 | loss: 3.253339\n",
      "epoch: 0 | individual: 217 | loss: 3.245963\n",
      "epoch: 0 | individual: 217 | loss: 3.239120\n",
      "epoch: 0 | individual: 217 | loss: 3.234906\n",
      "epoch: 0 | individual: 217 | loss: 3.320727\n",
      "epoch: 0 | individual: 217 | loss: 3.339798\n",
      "epoch: 0 | individual: 218 | loss: 3.268372\n",
      "epoch: 0 | individual: 218 | loss: 3.268354\n",
      "epoch: 0 | individual: 218 | loss: 3.223823\n",
      "epoch: 0 | individual: 218 | loss: 3.482960\n",
      "epoch: 0 | individual: 218 | loss: 3.558328\n",
      "epoch: 0 | individual: 218 | loss: 3.253667\n",
      "epoch: 0 | individual: 219 | loss: 3.216711\n",
      "epoch: 0 | individual: 219 | loss: 3.217590\n",
      "epoch: 0 | individual: 219 | loss: 3.216662\n",
      "epoch: 0 | individual: 219 | loss: 3.418909\n",
      "epoch: 0 | individual: 219 | loss: 3.253970\n",
      "epoch: 0 | individual: 219 | loss: 3.219908\n",
      "epoch: 0 | individual: 220 | loss: 3.222113\n",
      "epoch: 0 | individual: 220 | loss: 3.222347\n",
      "epoch: 0 | individual: 220 | loss: 3.221857\n",
      "epoch: 0 | individual: 220 | loss: 3.232792\n",
      "epoch: 0 | individual: 220 | loss: 3.209536\n",
      "epoch: 0 | individual: 220 | loss: 3.253605\n",
      "epoch: 0 | individual: 221 | loss: 3.208299\n",
      "epoch: 0 | individual: 221 | loss: 3.214133\n",
      "epoch: 0 | individual: 221 | loss: 3.376708\n",
      "epoch: 0 | individual: 221 | loss: 3.376605\n",
      "epoch: 0 | individual: 221 | loss: 3.276394\n",
      "epoch: 0 | individual: 221 | loss: 3.287502\n",
      "epoch: 0 | individual: 222 | loss: 3.203646\n",
      "epoch: 0 | individual: 222 | loss: 3.253523\n",
      "epoch: 0 | individual: 222 | loss: 3.243521\n",
      "epoch: 0 | individual: 222 | loss: 3.236518\n",
      "epoch: 0 | individual: 222 | loss: 3.325455\n",
      "epoch: 0 | individual: 222 | loss: 3.252630\n",
      "epoch: 0 | individual: 223 | loss: 3.560357\n",
      "epoch: 0 | individual: 223 | loss: 4.345949\n",
      "epoch: 0 | individual: 223 | loss: 4.539011\n",
      "epoch: 0 | individual: 223 | loss: 3.261666\n",
      "epoch: 0 | individual: 223 | loss: 3.220754\n",
      "epoch: 0 | individual: 223 | loss: 3.218300\n",
      "epoch: 0 | individual: 224 | loss: 3.205774\n",
      "epoch: 0 | individual: 224 | loss: 3.250694\n",
      "epoch: 0 | individual: 224 | loss: 3.242079\n",
      "epoch: 0 | individual: 224 | loss: 3.236308\n",
      "epoch: 0 | individual: 224 | loss: 3.237328\n",
      "epoch: 0 | individual: 224 | loss: 3.243736\n",
      "epoch: 0 | individual: 225 | loss: 3.207370\n",
      "epoch: 0 | individual: 225 | loss: 3.528141\n",
      "epoch: 0 | individual: 225 | loss: 3.884985\n",
      "epoch: 0 | individual: 225 | loss: 3.660147\n",
      "epoch: 0 | individual: 225 | loss: 3.286526\n",
      "epoch: 0 | individual: 225 | loss: 3.217609\n",
      "epoch: 0 | individual: 226 | loss: 3.412943\n",
      "epoch: 0 | individual: 226 | loss: 3.208357\n",
      "epoch: 0 | individual: 226 | loss: 3.266375\n",
      "epoch: 0 | individual: 226 | loss: 3.202356\n",
      "epoch: 0 | individual: 226 | loss: 3.328977\n",
      "epoch: 0 | individual: 226 | loss: 3.204181\n",
      "epoch: 0 | individual: 227 | loss: 3.219104\n",
      "epoch: 0 | individual: 227 | loss: 3.421266\n",
      "epoch: 0 | individual: 227 | loss: 3.676783\n",
      "epoch: 0 | individual: 227 | loss: 4.071062\n",
      "epoch: 0 | individual: 227 | loss: 3.257854\n",
      "epoch: 0 | individual: 227 | loss: 3.249912\n",
      "epoch: 0 | individual: 228 | loss: 3.250098\n",
      "epoch: 0 | individual: 228 | loss: 3.247051\n",
      "epoch: 0 | individual: 228 | loss: 3.269440\n",
      "epoch: 0 | individual: 228 | loss: 3.222753\n",
      "epoch: 0 | individual: 228 | loss: 3.365325\n",
      "epoch: 0 | individual: 228 | loss: 3.198098\n",
      "epoch: 0 | individual: 229 | loss: 3.217290\n",
      "epoch: 0 | individual: 229 | loss: 3.217666\n",
      "epoch: 0 | individual: 229 | loss: 3.267780\n",
      "epoch: 0 | individual: 229 | loss: 3.263098\n",
      "epoch: 0 | individual: 229 | loss: 3.396880\n",
      "epoch: 0 | individual: 229 | loss: 3.250376\n",
      "epoch: 0 | individual: 230 | loss: 3.264956\n",
      "epoch: 0 | individual: 230 | loss: 3.225724\n",
      "epoch: 0 | individual: 230 | loss: 3.501304\n",
      "epoch: 0 | individual: 230 | loss: 3.377700\n",
      "epoch: 0 | individual: 230 | loss: 3.271514\n",
      "epoch: 0 | individual: 230 | loss: 3.265012\n",
      "epoch: 0 | individual: 231 | loss: 3.432941\n",
      "epoch: 0 | individual: 231 | loss: 3.392870\n",
      "epoch: 0 | individual: 231 | loss: 3.208569\n",
      "epoch: 0 | individual: 231 | loss: 3.253684\n",
      "epoch: 0 | individual: 231 | loss: 3.252100\n",
      "epoch: 0 | individual: 231 | loss: 3.347751\n",
      "epoch: 0 | individual: 232 | loss: 3.203387\n",
      "epoch: 0 | individual: 232 | loss: 3.320316\n",
      "epoch: 0 | individual: 232 | loss: 3.203536\n",
      "epoch: 0 | individual: 232 | loss: 3.254512\n",
      "epoch: 0 | individual: 232 | loss: 3.226387\n",
      "epoch: 0 | individual: 232 | loss: 3.203368\n",
      "epoch: 0 | individual: 233 | loss: 3.324965\n",
      "epoch: 0 | individual: 233 | loss: 3.254282\n",
      "epoch: 0 | individual: 233 | loss: 3.242128\n",
      "epoch: 0 | individual: 233 | loss: 3.301762\n",
      "epoch: 0 | individual: 233 | loss: 3.253663\n",
      "epoch: 0 | individual: 233 | loss: 3.330580\n",
      "epoch: 0 | individual: 234 | loss: 3.219335\n",
      "epoch: 0 | individual: 234 | loss: 3.219362\n",
      "epoch: 0 | individual: 234 | loss: 3.219297\n",
      "epoch: 0 | individual: 234 | loss: 3.248482\n",
      "epoch: 0 | individual: 234 | loss: 3.228688\n",
      "epoch: 0 | individual: 234 | loss: 3.205698\n",
      "epoch: 0 | individual: 235 | loss: 3.264156\n",
      "epoch: 0 | individual: 235 | loss: 3.261569\n",
      "epoch: 0 | individual: 235 | loss: 3.263996\n",
      "epoch: 0 | individual: 235 | loss: 3.218965\n",
      "epoch: 0 | individual: 235 | loss: 3.250997\n",
      "epoch: 0 | individual: 235 | loss: 3.226045\n",
      "epoch: 0 | individual: 236 | loss: 3.215158\n",
      "epoch: 0 | individual: 236 | loss: 3.221883\n",
      "epoch: 0 | individual: 236 | loss: 3.262467\n",
      "epoch: 0 | individual: 236 | loss: 3.262291\n",
      "epoch: 0 | individual: 236 | loss: 3.284868\n",
      "epoch: 0 | individual: 236 | loss: 3.253029\n",
      "epoch: 0 | individual: 237 | loss: 3.704373\n",
      "epoch: 0 | individual: 237 | loss: 3.345213\n",
      "epoch: 0 | individual: 237 | loss: 3.337101\n",
      "epoch: 0 | individual: 237 | loss: 3.251545\n",
      "epoch: 0 | individual: 237 | loss: 3.247739\n",
      "epoch: 0 | individual: 237 | loss: 3.340252\n",
      "epoch: 0 | individual: 238 | loss: 3.246130\n",
      "epoch: 0 | individual: 238 | loss: 3.244519\n",
      "epoch: 0 | individual: 238 | loss: 3.204910\n",
      "epoch: 0 | individual: 238 | loss: 3.254130\n",
      "epoch: 0 | individual: 238 | loss: 3.239327\n",
      "epoch: 0 | individual: 238 | loss: 3.206805\n",
      "epoch: 0 | individual: 239 | loss: 3.221521\n",
      "epoch: 0 | individual: 239 | loss: 3.253904\n",
      "epoch: 0 | individual: 239 | loss: 3.243374\n",
      "epoch: 0 | individual: 239 | loss: 3.236500\n",
      "epoch: 0 | individual: 239 | loss: 3.235673\n",
      "epoch: 0 | individual: 239 | loss: 3.240068\n",
      "epoch: 0 | individual: 240 | loss: 3.295719\n",
      "epoch: 0 | individual: 240 | loss: 3.205592\n",
      "epoch: 0 | individual: 240 | loss: 3.250546\n",
      "epoch: 0 | individual: 240 | loss: 3.236793\n",
      "epoch: 0 | individual: 240 | loss: 3.218922\n",
      "epoch: 0 | individual: 240 | loss: 3.249692\n",
      "epoch: 0 | individual: 241 | loss: 3.329214\n",
      "epoch: 0 | individual: 241 | loss: 3.334017\n",
      "epoch: 0 | individual: 241 | loss: 3.463647\n",
      "epoch: 0 | individual: 241 | loss: 3.207932\n",
      "epoch: 0 | individual: 241 | loss: 3.212626\n",
      "epoch: 0 | individual: 241 | loss: 3.278773\n",
      "epoch: 0 | individual: 242 | loss: 3.248921\n",
      "epoch: 0 | individual: 242 | loss: 3.291237\n",
      "epoch: 0 | individual: 242 | loss: 3.673934\n",
      "epoch: 0 | individual: 242 | loss: 5.093369\n",
      "epoch: 0 | individual: 242 | loss: 3.590250\n",
      "epoch: 0 | individual: 242 | loss: 3.224764\n",
      "epoch: 0 | individual: 243 | loss: 3.320616\n",
      "epoch: 0 | individual: 243 | loss: 3.247612\n",
      "epoch: 0 | individual: 243 | loss: 3.480239\n",
      "epoch: 0 | individual: 243 | loss: 5.300802\n",
      "epoch: 0 | individual: 243 | loss: 3.347096\n",
      "epoch: 0 | individual: 243 | loss: 3.380771\n",
      "epoch: 0 | individual: 244 | loss: 3.247372\n",
      "epoch: 0 | individual: 244 | loss: 3.215458\n",
      "epoch: 0 | individual: 244 | loss: 3.225086\n",
      "epoch: 0 | individual: 244 | loss: 3.246905\n",
      "epoch: 0 | individual: 244 | loss: 3.233666\n",
      "epoch: 0 | individual: 244 | loss: 3.234621\n",
      "epoch: 0 | individual: 245 | loss: 3.220556\n",
      "epoch: 0 | individual: 245 | loss: 3.217837\n",
      "epoch: 0 | individual: 245 | loss: 3.218153\n",
      "epoch: 0 | individual: 245 | loss: 3.248497\n",
      "epoch: 0 | individual: 245 | loss: 3.459549\n",
      "epoch: 0 | individual: 245 | loss: 3.245923\n",
      "epoch: 0 | individual: 246 | loss: 3.266063\n",
      "epoch: 0 | individual: 246 | loss: 3.266107\n",
      "epoch: 0 | individual: 246 | loss: 3.266186\n",
      "epoch: 0 | individual: 246 | loss: 3.265332\n",
      "epoch: 0 | individual: 246 | loss: 3.279756\n",
      "epoch: 0 | individual: 246 | loss: 3.214293\n",
      "epoch: 0 | individual: 247 | loss: 3.238860\n",
      "epoch: 0 | individual: 247 | loss: 3.237233\n",
      "epoch: 0 | individual: 247 | loss: 3.263099\n",
      "epoch: 0 | individual: 247 | loss: 3.245162\n",
      "epoch: 0 | individual: 247 | loss: 3.234652\n",
      "epoch: 0 | individual: 247 | loss: 3.210683\n",
      "epoch: 0 | individual: 248 | loss: 3.224178\n",
      "epoch: 0 | individual: 248 | loss: 3.245914\n",
      "epoch: 0 | individual: 248 | loss: 3.254031\n",
      "epoch: 0 | individual: 248 | loss: 3.376599\n",
      "epoch: 0 | individual: 248 | loss: 3.463570\n",
      "epoch: 0 | individual: 248 | loss: 3.366608\n",
      "epoch: 0 | individual: 249 | loss: 3.337515\n",
      "epoch: 0 | individual: 249 | loss: 3.206033\n",
      "epoch: 0 | individual: 249 | loss: 3.342546\n",
      "epoch: 0 | individual: 249 | loss: 3.336152\n",
      "epoch: 0 | individual: 249 | loss: 3.247495\n",
      "epoch: 0 | individual: 249 | loss: 3.206527\n",
      "epoch: 0 | individual: 250 | loss: 3.240317\n",
      "epoch: 0 | individual: 250 | loss: 3.240408\n",
      "epoch: 0 | individual: 250 | loss: 3.239083\n",
      "epoch: 0 | individual: 250 | loss: 3.316748\n",
      "epoch: 0 | individual: 250 | loss: 3.602935\n",
      "epoch: 0 | individual: 250 | loss: 3.438991\n",
      "epoch: 0 | individual: 251 | loss: 3.518315\n",
      "epoch: 0 | individual: 251 | loss: 3.401629\n",
      "epoch: 0 | individual: 251 | loss: 3.240721\n",
      "epoch: 0 | individual: 251 | loss: 3.534833\n",
      "epoch: 0 | individual: 251 | loss: 3.308126\n",
      "epoch: 0 | individual: 251 | loss: 3.228741\n",
      "epoch: 0 | individual: 252 | loss: 3.240789\n",
      "epoch: 0 | individual: 252 | loss: 3.240984\n",
      "epoch: 0 | individual: 252 | loss: 3.423210\n",
      "epoch: 0 | individual: 252 | loss: 4.139704\n",
      "epoch: 0 | individual: 252 | loss: 3.301374\n",
      "epoch: 0 | individual: 252 | loss: 3.309654\n",
      "epoch: 0 | individual: 253 | loss: 3.241521\n",
      "epoch: 0 | individual: 253 | loss: 3.241791\n",
      "epoch: 0 | individual: 253 | loss: 3.239844\n",
      "epoch: 0 | individual: 253 | loss: 3.215853\n",
      "epoch: 0 | individual: 253 | loss: 3.239707\n",
      "epoch: 0 | individual: 253 | loss: 3.239392\n",
      "epoch: 0 | individual: 254 | loss: 3.217316\n",
      "epoch: 0 | individual: 254 | loss: 3.272232\n",
      "epoch: 0 | individual: 254 | loss: 3.361333\n",
      "epoch: 0 | individual: 254 | loss: 3.230323\n",
      "epoch: 0 | individual: 254 | loss: 3.435149\n",
      "epoch: 0 | individual: 254 | loss: 3.704454\n",
      "epoch: 0 | individual: 255 | loss: 3.527627\n",
      "epoch: 0 | individual: 255 | loss: 3.632377\n",
      "epoch: 0 | individual: 255 | loss: 3.352433\n",
      "epoch: 0 | individual: 255 | loss: 3.552608\n",
      "epoch: 0 | individual: 255 | loss: 3.224563\n",
      "epoch: 0 | individual: 255 | loss: 3.246084\n",
      "epoch: 0 | individual: 256 | loss: 3.246163\n",
      "epoch: 0 | individual: 256 | loss: 3.211474\n",
      "epoch: 0 | individual: 256 | loss: 3.216094\n",
      "epoch: 0 | individual: 256 | loss: 3.245383\n",
      "epoch: 0 | individual: 256 | loss: 3.235864\n",
      "epoch: 0 | individual: 256 | loss: 3.236118\n",
      "epoch: 0 | individual: 257 | loss: 3.229584\n",
      "epoch: 0 | individual: 257 | loss: 3.244557\n",
      "epoch: 0 | individual: 257 | loss: 3.308817\n",
      "epoch: 0 | individual: 257 | loss: 3.433848\n",
      "epoch: 0 | individual: 257 | loss: 3.391960\n",
      "epoch: 0 | individual: 257 | loss: 3.847096\n",
      "epoch: 0 | individual: 258 | loss: 3.230232\n",
      "epoch: 0 | individual: 258 | loss: 3.242589\n",
      "epoch: 0 | individual: 258 | loss: 3.460656\n",
      "epoch: 0 | individual: 258 | loss: 3.223863\n",
      "epoch: 0 | individual: 258 | loss: 3.240764\n",
      "epoch: 0 | individual: 258 | loss: 3.234700\n",
      "epoch: 0 | individual: 259 | loss: 3.464217\n",
      "epoch: 0 | individual: 259 | loss: 4.109864\n",
      "epoch: 0 | individual: 259 | loss: 3.547508\n",
      "epoch: 0 | individual: 259 | loss: 3.319117\n",
      "epoch: 0 | individual: 259 | loss: 3.240971\n",
      "epoch: 0 | individual: 259 | loss: 3.235028\n",
      "epoch: 0 | individual: 260 | loss: 3.236690\n",
      "epoch: 0 | individual: 260 | loss: 3.722977\n",
      "epoch: 0 | individual: 260 | loss: 3.969428\n",
      "epoch: 0 | individual: 260 | loss: 4.156097\n",
      "epoch: 0 | individual: 260 | loss: 4.903839\n",
      "epoch: 0 | individual: 260 | loss: 3.240533\n",
      "epoch: 0 | individual: 261 | loss: 3.236654\n",
      "epoch: 0 | individual: 261 | loss: 3.679451\n",
      "epoch: 0 | individual: 261 | loss: 3.240263\n",
      "epoch: 0 | individual: 261 | loss: 3.232326\n",
      "epoch: 0 | individual: 261 | loss: 3.230774\n",
      "epoch: 0 | individual: 261 | loss: 3.481267\n",
      "epoch: 0 | individual: 262 | loss: 3.270800\n",
      "epoch: 0 | individual: 262 | loss: 3.296399\n",
      "epoch: 0 | individual: 262 | loss: 3.306743\n",
      "epoch: 0 | individual: 262 | loss: 3.414453\n",
      "epoch: 0 | individual: 262 | loss: 3.265906\n",
      "epoch: 0 | individual: 262 | loss: 3.258206\n",
      "epoch: 0 | individual: 263 | loss: 3.234121\n",
      "epoch: 0 | individual: 263 | loss: 3.233900\n",
      "epoch: 0 | individual: 263 | loss: 3.234061\n",
      "epoch: 0 | individual: 263 | loss: 3.233437\n",
      "epoch: 0 | individual: 263 | loss: 3.236713\n",
      "epoch: 0 | individual: 263 | loss: 3.234236\n",
      "epoch: 0 | individual: 264 | loss: 3.228478\n",
      "epoch: 0 | individual: 264 | loss: 3.269960\n",
      "epoch: 0 | individual: 264 | loss: 3.228480\n",
      "epoch: 0 | individual: 264 | loss: 3.228428\n",
      "epoch: 0 | individual: 264 | loss: 3.225929\n",
      "epoch: 0 | individual: 264 | loss: 3.240241\n",
      "epoch: 0 | individual: 265 | loss: 3.227233\n",
      "epoch: 0 | individual: 265 | loss: 3.218899\n",
      "epoch: 0 | individual: 265 | loss: 3.230875\n",
      "epoch: 0 | individual: 265 | loss: 3.389667\n",
      "epoch: 0 | individual: 265 | loss: 4.013785\n",
      "epoch: 0 | individual: 265 | loss: 3.431113\n",
      "epoch: 0 | individual: 266 | loss: 3.229658\n",
      "epoch: 0 | individual: 266 | loss: 3.280336\n",
      "epoch: 0 | individual: 266 | loss: 3.447779\n",
      "epoch: 0 | individual: 266 | loss: 3.421623\n",
      "epoch: 0 | individual: 266 | loss: 3.331498\n",
      "epoch: 0 | individual: 266 | loss: 3.240662\n",
      "epoch: 0 | individual: 267 | loss: 3.232499\n",
      "epoch: 0 | individual: 267 | loss: 3.232975\n",
      "epoch: 0 | individual: 267 | loss: 3.239253\n",
      "epoch: 0 | individual: 267 | loss: 3.283920\n",
      "epoch: 0 | individual: 267 | loss: 3.237540\n",
      "epoch: 0 | individual: 267 | loss: 3.234387\n",
      "epoch: 0 | individual: 268 | loss: 3.240988\n",
      "epoch: 0 | individual: 268 | loss: 3.229753\n",
      "epoch: 0 | individual: 268 | loss: 3.533802\n",
      "epoch: 0 | individual: 268 | loss: 4.229812\n",
      "epoch: 0 | individual: 268 | loss: 3.267631\n",
      "epoch: 0 | individual: 268 | loss: 3.240153\n",
      "epoch: 0 | individual: 269 | loss: 3.234768\n",
      "epoch: 0 | individual: 269 | loss: 3.235013\n",
      "epoch: 0 | individual: 269 | loss: 3.234103\n",
      "epoch: 0 | individual: 269 | loss: 3.357434\n",
      "epoch: 0 | individual: 269 | loss: 3.238943\n",
      "epoch: 0 | individual: 269 | loss: 3.232304\n",
      "epoch: 0 | individual: 270 | loss: 3.231623\n",
      "epoch: 0 | individual: 270 | loss: 3.233407\n",
      "epoch: 0 | individual: 270 | loss: 3.288567\n",
      "epoch: 0 | individual: 270 | loss: 3.311326\n",
      "epoch: 0 | individual: 270 | loss: 3.327908\n",
      "epoch: 0 | individual: 270 | loss: 3.237042\n",
      "epoch: 0 | individual: 271 | loss: 3.218264\n",
      "epoch: 0 | individual: 271 | loss: 3.218292\n",
      "epoch: 0 | individual: 271 | loss: 3.218018\n",
      "epoch: 0 | individual: 271 | loss: 3.340149\n",
      "epoch: 0 | individual: 271 | loss: 3.236546\n",
      "epoch: 0 | individual: 271 | loss: 3.233711\n",
      "epoch: 0 | individual: 272 | loss: 3.421166\n",
      "epoch: 0 | individual: 272 | loss: 3.393376\n",
      "epoch: 0 | individual: 272 | loss: 3.336013\n",
      "epoch: 0 | individual: 272 | loss: 3.528946\n",
      "epoch: 0 | individual: 272 | loss: 3.395097\n",
      "epoch: 0 | individual: 272 | loss: 3.236557\n",
      "epoch: 0 | individual: 273 | loss: 3.235061\n",
      "epoch: 0 | individual: 273 | loss: 3.234987\n",
      "epoch: 0 | individual: 273 | loss: 3.329923\n",
      "epoch: 0 | individual: 273 | loss: 3.226253\n",
      "epoch: 0 | individual: 273 | loss: 3.225792\n",
      "epoch: 0 | individual: 273 | loss: 3.235369\n",
      "epoch: 0 | individual: 274 | loss: 3.291946\n",
      "epoch: 0 | individual: 274 | loss: 3.476468\n",
      "epoch: 0 | individual: 274 | loss: 3.314134\n",
      "epoch: 0 | individual: 274 | loss: 3.440133\n",
      "epoch: 0 | individual: 274 | loss: 3.235916\n",
      "epoch: 0 | individual: 274 | loss: 3.232698\n",
      "epoch: 0 | individual: 275 | loss: 3.386497\n",
      "epoch: 0 | individual: 275 | loss: 3.232088\n",
      "epoch: 0 | individual: 275 | loss: 3.279130\n",
      "epoch: 0 | individual: 275 | loss: 3.236637\n",
      "epoch: 0 | individual: 275 | loss: 3.311091\n",
      "epoch: 0 | individual: 275 | loss: 3.216638\n",
      "epoch: 0 | individual: 276 | loss: 3.268313\n",
      "epoch: 0 | individual: 276 | loss: 3.293497\n",
      "epoch: 0 | individual: 276 | loss: 3.236483\n",
      "epoch: 0 | individual: 276 | loss: 3.200912\n",
      "epoch: 0 | individual: 276 | loss: 3.236608\n",
      "epoch: 0 | individual: 276 | loss: 3.233107\n",
      "epoch: 0 | individual: 277 | loss: 3.205810\n",
      "epoch: 0 | individual: 277 | loss: 3.229877\n",
      "epoch: 0 | individual: 277 | loss: 3.250674\n",
      "epoch: 0 | individual: 277 | loss: 3.429579\n",
      "epoch: 0 | individual: 277 | loss: 3.694384\n",
      "epoch: 0 | individual: 277 | loss: 3.247462\n",
      "epoch: 0 | individual: 278 | loss: 3.334989\n",
      "epoch: 0 | individual: 278 | loss: 3.236417\n",
      "epoch: 0 | individual: 278 | loss: 3.233644\n",
      "epoch: 0 | individual: 278 | loss: 3.236459\n",
      "epoch: 0 | individual: 278 | loss: 3.232259\n",
      "epoch: 0 | individual: 278 | loss: 3.232603\n",
      "epoch: 0 | individual: 279 | loss: 3.402486\n",
      "epoch: 0 | individual: 279 | loss: 3.271702\n",
      "epoch: 0 | individual: 279 | loss: 3.234859\n",
      "epoch: 0 | individual: 279 | loss: 3.293118\n",
      "epoch: 0 | individual: 279 | loss: 3.249883\n",
      "epoch: 0 | individual: 279 | loss: 3.345091\n",
      "epoch: 0 | individual: 280 | loss: 3.234947\n",
      "epoch: 0 | individual: 280 | loss: 3.234987\n",
      "epoch: 0 | individual: 280 | loss: 3.201188\n",
      "epoch: 0 | individual: 280 | loss: 3.209046\n",
      "epoch: 0 | individual: 280 | loss: 3.236734\n",
      "epoch: 0 | individual: 280 | loss: 3.233698\n",
      "epoch: 0 | individual: 281 | loss: 3.443448\n",
      "epoch: 0 | individual: 281 | loss: 3.829132\n",
      "epoch: 0 | individual: 281 | loss: 3.277727\n",
      "epoch: 0 | individual: 281 | loss: 3.413822\n",
      "epoch: 0 | individual: 281 | loss: 3.236417\n",
      "epoch: 0 | individual: 281 | loss: 3.233395\n",
      "epoch: 0 | individual: 282 | loss: 3.206185\n",
      "epoch: 0 | individual: 282 | loss: 3.236207\n",
      "epoch: 0 | individual: 282 | loss: 3.206123\n",
      "epoch: 0 | individual: 282 | loss: 3.212178\n",
      "epoch: 0 | individual: 282 | loss: 3.235721\n",
      "epoch: 0 | individual: 282 | loss: 3.232402\n",
      "epoch: 0 | individual: 283 | loss: 3.233503\n",
      "epoch: 0 | individual: 283 | loss: 3.233190\n",
      "epoch: 0 | individual: 283 | loss: 3.221598\n",
      "epoch: 0 | individual: 283 | loss: 3.234563\n",
      "epoch: 0 | individual: 283 | loss: 3.230289\n",
      "epoch: 0 | individual: 283 | loss: 3.230474\n",
      "epoch: 0 | individual: 284 | loss: 3.226197\n",
      "epoch: 0 | individual: 284 | loss: 3.203830\n",
      "epoch: 0 | individual: 284 | loss: 3.232925\n",
      "epoch: 0 | individual: 284 | loss: 3.246006\n",
      "epoch: 0 | individual: 284 | loss: 3.237536\n",
      "epoch: 0 | individual: 284 | loss: 3.275195\n",
      "epoch: 0 | individual: 285 | loss: 3.232071\n",
      "epoch: 0 | individual: 285 | loss: 3.223674\n",
      "epoch: 0 | individual: 285 | loss: 3.416737\n",
      "epoch: 0 | individual: 285 | loss: 3.231640\n",
      "epoch: 0 | individual: 285 | loss: 3.227779\n",
      "epoch: 0 | individual: 285 | loss: 3.228049\n",
      "epoch: 0 | individual: 286 | loss: 3.551588\n",
      "epoch: 0 | individual: 286 | loss: 3.667695\n",
      "epoch: 0 | individual: 286 | loss: 3.229714\n",
      "epoch: 0 | individual: 286 | loss: 3.235934\n",
      "epoch: 0 | individual: 286 | loss: 3.346607\n",
      "epoch: 0 | individual: 286 | loss: 3.337788\n",
      "epoch: 0 | individual: 287 | loss: 3.233454\n",
      "epoch: 0 | individual: 287 | loss: 3.233408\n",
      "epoch: 0 | individual: 287 | loss: 3.318812\n",
      "epoch: 0 | individual: 287 | loss: 3.233174\n",
      "epoch: 0 | individual: 287 | loss: 3.206425\n",
      "epoch: 0 | individual: 287 | loss: 3.231597\n",
      "epoch: 0 | individual: 288 | loss: 3.231610\n",
      "epoch: 0 | individual: 288 | loss: 3.228943\n",
      "epoch: 0 | individual: 288 | loss: 3.226918\n",
      "epoch: 0 | individual: 288 | loss: 3.305374\n",
      "epoch: 0 | individual: 288 | loss: 3.222021\n",
      "epoch: 0 | individual: 288 | loss: 3.274902\n",
      "epoch: 0 | individual: 289 | loss: 3.200564\n",
      "epoch: 0 | individual: 289 | loss: 3.231504\n",
      "epoch: 0 | individual: 289 | loss: 3.200685\n",
      "epoch: 0 | individual: 289 | loss: 3.247570\n",
      "epoch: 0 | individual: 289 | loss: 3.394422\n",
      "epoch: 0 | individual: 289 | loss: 3.201645\n",
      "epoch: 0 | individual: 290 | loss: 3.228883\n",
      "epoch: 0 | individual: 290 | loss: 3.228469\n",
      "epoch: 0 | individual: 290 | loss: 3.226682\n",
      "epoch: 0 | individual: 290 | loss: 3.437140\n",
      "epoch: 0 | individual: 290 | loss: 3.231181\n",
      "epoch: 0 | individual: 290 | loss: 3.226824\n",
      "epoch: 0 | individual: 291 | loss: 3.240093\n",
      "epoch: 0 | individual: 291 | loss: 3.391339\n",
      "epoch: 0 | individual: 291 | loss: 3.562585\n",
      "epoch: 0 | individual: 291 | loss: 3.240355\n",
      "epoch: 0 | individual: 291 | loss: 3.257792\n",
      "epoch: 0 | individual: 291 | loss: 3.230505\n",
      "epoch: 0 | individual: 292 | loss: 3.258369\n",
      "epoch: 0 | individual: 292 | loss: 3.257741\n",
      "epoch: 0 | individual: 292 | loss: 3.257670\n",
      "epoch: 0 | individual: 292 | loss: 3.222766\n",
      "epoch: 0 | individual: 292 | loss: 3.200532\n",
      "epoch: 0 | individual: 292 | loss: 3.228363\n",
      "epoch: 0 | individual: 293 | loss: 3.239178\n",
      "epoch: 0 | individual: 293 | loss: 3.237098\n",
      "epoch: 0 | individual: 293 | loss: 3.236893\n",
      "epoch: 0 | individual: 293 | loss: 3.383753\n",
      "epoch: 0 | individual: 293 | loss: 3.235204\n",
      "epoch: 0 | individual: 293 | loss: 3.244728\n",
      "epoch: 0 | individual: 294 | loss: 3.344124\n",
      "epoch: 0 | individual: 294 | loss: 3.210571\n",
      "epoch: 0 | individual: 294 | loss: 3.227948\n",
      "epoch: 0 | individual: 294 | loss: 3.207239\n",
      "epoch: 0 | individual: 294 | loss: 3.223557\n",
      "epoch: 0 | individual: 294 | loss: 3.470185\n",
      "epoch: 0 | individual: 295 | loss: 3.242125\n",
      "epoch: 0 | individual: 295 | loss: 3.297687\n",
      "epoch: 0 | individual: 295 | loss: 3.324496\n",
      "epoch: 0 | individual: 295 | loss: 3.295735\n",
      "epoch: 0 | individual: 295 | loss: 3.234412\n",
      "epoch: 0 | individual: 295 | loss: 3.228718\n",
      "epoch: 0 | individual: 296 | loss: 3.213778\n",
      "epoch: 0 | individual: 296 | loss: 3.212954\n",
      "epoch: 0 | individual: 296 | loss: 3.212883\n",
      "epoch: 0 | individual: 296 | loss: 3.213609\n",
      "epoch: 0 | individual: 296 | loss: 3.239815\n",
      "epoch: 0 | individual: 296 | loss: 3.218211\n",
      "epoch: 0 | individual: 297 | loss: 3.220124\n",
      "epoch: 0 | individual: 297 | loss: 3.456403\n",
      "epoch: 0 | individual: 297 | loss: 3.273281\n",
      "epoch: 0 | individual: 297 | loss: 3.377483\n",
      "epoch: 0 | individual: 297 | loss: 3.380659\n",
      "epoch: 0 | individual: 297 | loss: 3.290799\n",
      "epoch: 0 | individual: 298 | loss: 3.290055\n",
      "epoch: 0 | individual: 298 | loss: 3.594449\n",
      "epoch: 0 | individual: 298 | loss: 3.714490\n",
      "epoch: 0 | individual: 298 | loss: 3.333793\n",
      "epoch: 0 | individual: 298 | loss: 3.205557\n",
      "epoch: 0 | individual: 298 | loss: 3.238058\n",
      "epoch: 0 | individual: 299 | loss: 3.318607\n",
      "epoch: 0 | individual: 299 | loss: 3.239252\n",
      "epoch: 0 | individual: 299 | loss: 3.230026\n",
      "epoch: 0 | individual: 299 | loss: 3.208560\n",
      "epoch: 0 | individual: 299 | loss: 3.239668\n",
      "epoch: 0 | individual: 299 | loss: 3.231949\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNNmodel()\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()  #(y_hat, y)\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1\n",
    "seq_length = 7\n",
    "\n",
    "c1_penalty_weight = 10\n",
    "\n",
    "for epochi in range(epochs):\n",
    "    #print(epochi)\n",
    "    for individual_i in range(300):\n",
    "        #print(f\"Individual: {individual_i}\")\n",
    "\n",
    "        travel_diary = X[individual_i, :, 0, :].unsqueeze(1).to(torch.float32)\n",
    "\n",
    "        for i in range(1, travel_diary.shape[0] - seq_length):\n",
    "\n",
    "            # Conditions of violation\n",
    "\n",
    "\n",
    "\n",
    "            sliding_input = travel_diary[i:seq_length+i,0,:].unsqueeze(1)\n",
    "\n",
    "            #print(sliding_input.shape)\n",
    "\n",
    "            y_tripstart_pred, y_tripend_pred, y_distance_pred, y_purpouse_pred = rnn_model.forward(sliding_input)\n",
    "\n",
    "            y_tripstart_true, y_tripend_true, y_distance_true, y_purpouse_true = y_cont[individual_i,i,:10], y_cont[individual_i,i,10:20], y_cont[individual_i,i,20:], y_cat[individual_i,i,:].long()\n",
    "\n",
    "            # conditions\n",
    "            c1 = torch.clamp(y_tripstart_pred - y_tripend_pred, min=0)\n",
    "            #print(c1)\n",
    "            c1_loss = c1_penalty_weight * torch.mean(c1)\n",
    "\n",
    "\n",
    "            categorical_prediction = torch.argmax(y_purpouse_pred, dim=-1)\n",
    "            \n",
    "\n",
    "            # Compute MSE Loss across the full week\n",
    "            tripstart_loss = mse_loss(y_tripstart_pred, y_tripstart_true)\n",
    "            tripend_loss = mse_loss(y_tripend_pred, y_tripend_true)\n",
    "            distance_loss = mse_loss(y_distance_pred, y_distance_true)\n",
    "            purpouse_loss = ce_loss(y_purpouse_pred, y_purpouse_true)\n",
    "\n",
    "\n",
    "            #print(f\"Truth: {y_cat_i}\")\n",
    "            #print(f\"Prediction: {categorical_prediction}\") \n",
    "            #print(y_cat_hat_logit)\n",
    "\n",
    "            #print(f\"TripStart pred: {y_tripstart_pred}\")\n",
    "            #print(f\"TripEnd pred: {y_tripend_pred}\")\n",
    "            #print(\"\")\n",
    "            #print(f\"TripStart true: {y_tripstart_true}\")\n",
    "            #print(f\"TripEnd true: {y_tripend_true}\")\n",
    "            #print(\"\")\n",
    "            #print(y_cat_i.shape)\n",
    "            #print(y_cat_hat_logit.shape)\n",
    "            #print(y_cont_i.shape)\n",
    "            #print(y_cont_hat.shape)\n",
    "            #print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            combined_loss = tripstart_loss + tripend_loss + distance_loss + purpouse_loss + c1_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            combined_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            print(f\"epoch: {epochi} | individual: {individual_i} | loss: {combined_loss:2f}\")\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLEconPS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
