{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "from Modules.Loader_wrangler import *\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure basic logging\n",
    "logging.basicConfig(level=logging.INFO, force=True, format='%(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m play \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerged_df2017.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurvey_year\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2017\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Cambridge24.25/D200_ML_econ/ProblemSets/Project/Modules/Loader_wrangler.py:306\u001b[0m, in \u001b[0;36mloader\u001b[0;34m(output_file_name, wrangle_func, nts_trip, nts_vehicle, nts_i, nts_household, nts_psu, nts_day, chunksize, sample_size, survey_year)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03mLoads, merges, and processes National Travel Survey (NTS) datasets in chunks.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m    A fully merged and processed DataFrame containing travel data.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# Load in vehicle df\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m vehicle_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnts_vehicle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Load in Individual df\u001b[39;00m\n\u001b[1;32m    310\u001b[0m i_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(nts_i, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,  dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/MLEconPS2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLEconPS2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/MLEconPS2/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/MLEconPS2/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "play = loader(output_file_name=\"merged_df2017.pkl\", chunksize=100000, sample_size=100000, survey_year=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = pd.read_pickle(\"/home/trapfishscott/Cambridge24.25/D200_ML_econ/ProblemSets/Project/data/merged_df2017.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining only relevant variables and making into a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_vars = [\"TWSMonth\", \"TravelYear\", \"TravelWeekDay_B01ID\"]\n",
    "individual_vars =[\"PSUGOR_B02ID\", \"IndIncome2002_B02ID\", \"HHoldNumChildren\", \"VehMakeModel_B02ID\"]\n",
    "\n",
    "outcome_vars = [\"TripStart\", \"TripEnd\", \"TripDisExSW\", \"TripPurpose_B01ID\"]\n",
    "extra_vars = [\"IndividualID_x\", \"JourSeq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = play[extra_vars + individual_vars + temporal_vars + outcome_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = ts_df.sort_values([\"IndividualID_x\", \"TravelWeekDay_B01ID\", \"JourSeq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most weekly travel ~ 67\n",
      "Average weekly travel ~ 14.4106463878327\n",
      "97th percentile of weekly travel ~ 34.0\n"
     ]
    }
   ],
   "source": [
    "weekly_travel = []\n",
    "\n",
    "\n",
    "for i in ts_df[\"IndividualID_x\"].unique():\n",
    "    i_df = ts_df[ts_df[\"IndividualID_x\"] == i]\n",
    "    weekly_travel.append(len(i_df))\n",
    "\n",
    "max_weekly_travel = max(weekly_travel)\n",
    "mean_weekly_travel = sum(weekly_travel)/ len(weekly_travel)\n",
    "\n",
    "print(f\"Most weekly travel ~ {max_weekly_travel}\")\n",
    "print(f\"Average weekly travel ~ {mean_weekly_travel}\")\n",
    "\n",
    "\n",
    "percentile_97 = np.percentile(weekly_travel, 97)\n",
    "\n",
    "print(f\"97th percentile of weekly travel ~ {percentile_97}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_travel_weeks(df, use_masking=False):\n",
    "    df = df.copy()\n",
    "    df_chunks = []\n",
    "    full_week_encoding = list(range(0,9))\n",
    "    individual_ids = df[\"IndividualID_x\"].unique()\n",
    "\n",
    "    for i in individual_ids:\n",
    "        break_flag = False\n",
    "        i_df = df[df[\"IndividualID_x\"] == i]\n",
    "        #display(i_df)\n",
    "        #print(\"\")\n",
    "        included_travel_day = i_df[\"TravelWeekDay_B01ID\"].to_list()\n",
    "        #print(included_travel_day)\n",
    "        travel_day_no_drive = list(set(full_week_encoding) - set(included_travel_day))\n",
    "        #print(travel_day_no_drive)\n",
    "\n",
    "        idle_row = {}\n",
    "\n",
    "        imputed_travel_df = pd.DataFrame({\n",
    "            \"TravelWeekDay_B01ID\": travel_day_no_drive,\n",
    "            \"IndividualID_x\": [i]*len(travel_day_no_drive),\n",
    "            \"JourSeq\": [0]*len(travel_day_no_drive)\n",
    "        })\n",
    "\n",
    "        for col in i_df.columns:\n",
    "            if col in outcome_vars:\n",
    "                imputed_travel_df[col] = [0]*len(travel_day_no_drive)\n",
    "            if col in individual_vars:\n",
    "                if len(i_df[col].unique()) != 1:\n",
    "                    print(f\"{col} is erroneous for {i}\")\n",
    "                    print(f\"Unique vals: {i_df[col].unique()}\")\n",
    "                    break_flag = True\n",
    "                    break\n",
    "                else:\n",
    "                    imputed_travel_df[col] = i_df[col].unique()[0]\n",
    "                    idle_row[col] = i_df[col].unique()[0]\n",
    "\n",
    "            if col != \"TravelWeekDay_B01ID\" and col in temporal_vars:\n",
    "                if len(i_df[col].unique()) != 1:\n",
    "                    print(f\"{col} is erroneous for {i}\")\n",
    "                    print(f\"Unique vals: {i_df[col].unique()}\")\n",
    "                    break_flag = True\n",
    "                    break\n",
    "                else:\n",
    "                    imputed_travel_df[col] = i_df[col].unique()[0]\n",
    "                    idle_row[col] = i_df[col].unique()[0]\n",
    "\n",
    "        if break_flag:\n",
    "            print(\"Continuing to next individual\")\n",
    "            continue\n",
    "\n",
    "        full_df = pd.concat([i_df, imputed_travel_df])\n",
    "\n",
    "        if use_masking:\n",
    "            if len(full_df) < percentile_97:\n",
    "                rows_to_mask = int(percentile_97 - len(full_df))\n",
    "                #print(f\"Length of imputed df: {len(full_df)}\")\n",
    "                #print(f\"Number of rows to mask: {rows_to_mask}\")\n",
    "\n",
    "                new_rows = pd.DataFrame([{\n",
    "                    \"TravelWeekDay_B01ID\": 8,\n",
    "                    \"IndividualID_x\": i,\n",
    "                    \"JourSeq\": 0,\n",
    "                    **idle_row\n",
    "                    }]* rows_to_mask)\n",
    "                \n",
    "                #print(new_row)\n",
    "\n",
    "                full_df = pd.concat([full_df, new_rows], ignore_index=True)\n",
    "\n",
    "                #print(len(full_df))\n",
    "\n",
    "            if len(full_df) > percentile_97:\n",
    "                print(f\"Outlier individual with travel ~ {len(full_df)}\")\n",
    "                print(f\"Continuing...\")\n",
    "                continue\n",
    "\n",
    "        df_chunks.append(full_df)\n",
    "\n",
    "        #display(imputed_travel_df)\n",
    "        #print(\"\")\n",
    "        #display(full_df)\n",
    "\n",
    "    df_to_return = pd.concat(df_chunks)\n",
    "\n",
    "    df_to_return = df_to_return.sort_values([\"IndividualID_x\", \"TravelYear\", \"TWSMonth\", \"TravelWeekDay_B01ID\", \"JourSeq\", \"TripStart\", \"TripEnd\"] )[[\"IndividualID_x\", \"JourSeq\"] + outcome_vars + temporal_vars]\n",
    "\n",
    "    df_to_return.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    df_to_return.fillna(0, inplace=True)\n",
    "\n",
    "    return df_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TravelYear is erroneous for 2017014397.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017014398.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017014552.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017014714.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017014715.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017014773.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017014964.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017014965.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015043.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015044.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015045.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015046.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015097.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015137.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015147.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015165.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015191.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015192.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015193.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015196.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015249.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015258.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015305.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015342.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015362.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015387.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015388.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015390.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015397.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015398.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015477.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015479.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015496.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015562.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015569.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015579.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015581.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015582.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015588.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015590.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015602.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015624.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015625.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015642.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015644.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015659.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015667.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015668.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015671.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015701.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015704.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015726.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015756.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015765.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015770.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015800.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015811.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015860.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015862.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015867.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015868.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015915.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n",
      "TravelYear is erroneous for 2017015980.0\n",
      "Unique vals: [2018. 2017.]\n",
      "Continuing to next individual\n"
     ]
    }
   ],
   "source": [
    "df = impute_missing_travel_weeks(ts_df)\n",
    "\n",
    "# When we are moving from year to year. Probably not a huge issue but might fix later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LSTM\n",
    "\n",
    "def convert_to_tensor(df, seq_length, cols_to_drop, debug=True):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df.drop(columns=cols_to_drop, axis=1, errors=\"ignore\")\n",
    "\n",
    "    df_array = df.to_numpy()\n",
    "\n",
    "    # Drop observations to make it fit sequence length\n",
    "\n",
    "    rows_to_drop = df_array.shape[0] % seq_length\n",
    "\n",
    "    if rows_to_drop > 0:\n",
    "        df_array = df_array[:-rows_to_drop,:]\n",
    "\n",
    "    print(df_array.shape)\n",
    "\n",
    "    n = int(df_array.shape[0]/seq_length)\n",
    "    input_features = int(df_array.shape[1])\n",
    "\n",
    "    df_array = df_array.reshape((n, seq_length, input_features))\n",
    "\n",
    "    df_array = df_array.transpose(1,0,2)\n",
    "\n",
    "    print(f\"Reshaped (seq_length, n_batches, input_features): {df_array.shape}\")\n",
    "\n",
    "    # Ensuring the array transformation matches the df\n",
    "\n",
    "    if debug is True:\n",
    "\n",
    "        test_value = random.randint(0,df_array.shape[0])\n",
    "\n",
    "        assert np.array_equal(\n",
    "            df_array[:,test_value,:],\n",
    "            df.iloc[test_value*seq_length:test_value*seq_length+seq_length, :].to_numpy()\n",
    "        ), \"Mismatch between reshaped array and original df\"\n",
    "\n",
    "    tensor_data = torch.tensor(df_array, dtype=torch.float32)\n",
    "\n",
    "    return tensor_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126602, 8)\n",
      "Reshaped (seq_length, n_batches, input_features): (14, 9043, 8)\n"
     ]
    }
   ],
   "source": [
    "travel_tensor = convert_to_tensor(df=df, seq_length=14, cols_to_drop=[\"JourSeq\", \"NumTrips\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TripStart', 'TripEnd', 'TripDisExSW', 'TripPurpose_B01ID']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining parameters\n",
    "INPUT_SIZE = travel_tensor.shape[2]\n",
    "HIDDEN_SIZE = 3\n",
    "NUM_LAYERS = 1\n",
    "OUTPUT_SIZE = len(outcome_vars)\n",
    "\n",
    "outcome_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define RNN layer\n",
    "\n",
    "        self.rnn = nn.RNN(INPUT_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "        # Output layer\n",
    "\n",
    "        self.output = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        out, hh = self.rnn(X)\n",
    "\n",
    "        print(f\"out shape: {out.shape}\")\n",
    "        print(f\"hh shape: {hh.shape}\")\n",
    "\n",
    "        y_hat_vector = self.output(hh)\n",
    "\n",
    "        print(f\"y_hat shape: {y_hat_vector.shape}\")\n",
    "\n",
    "        print(y_hat_vector)\n",
    "\n",
    "        y_hat = {}\n",
    "\n",
    "        for index in range(y_hat_vector.shape[2]):\n",
    "            y_hat[index+1] = y_hat_vector[:,:,index].detach()\n",
    "\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 shape: torch.Size([14, 1, 8])\n",
      "out shape: torch.Size([14, 1, 3])\n",
      "hh shape: torch.Size([1, 1, 3])\n",
      "y_hat shape: torch.Size([1, 1, 4])\n",
      "tensor([[[ 0.5506, -0.3124,  0.2871, -0.1610]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: tensor([[0.5506]]),\n",
       " 2: tensor([[-0.3124]]),\n",
       " 3: tensor([[0.2871]]),\n",
       " 4: tensor([[-0.1610]])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking one test draw\n",
    "\n",
    "rnn_model = RNNmodel()\n",
    "\n",
    "X1 = travel_tensor[:,0,:].unsqueeze(1)\n",
    "print(f\"X1 shape: {X1.shape}\")\n",
    "\n",
    "y_hat = rnn_model.forward(X1)\n",
    "\n",
    "y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cat = nn.CrossEntropyLoss()  #(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM for single batch size\n",
    "\n",
    "# Looping over batch dimension\n",
    "\n",
    "for batch in range(travel_tensor.shape[1]):\n",
    "    print(travel_tensor[:,batch,:].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLEconPS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
